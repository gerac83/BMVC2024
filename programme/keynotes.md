---
layout: default_sparse
title: Keynotes
permalink: /programme/keynotes/
index: 5
---

<div class="row justify-content-around pl-4 pr-4">
    <div class="col-12">
        <!-- 1st keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Mubarak_Shah.png" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a></h4>
                    <span class=""><small>University of Central Florida</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Privacy Preservation and Bias Mitigation in Human Action Recognition</b></h5>
                    <p class="text-center mb-1"><small></small></p><br>
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>Advances in action recognition have enabled a wide range of real-world applications, e.g. elderly person monitoring systems, autonomous vehicles, sports analysis. As these techniques are being used in the real world two important issues have emerged: privacy and bias. Most of these video understanding applications involve extensive computation, for which a user needs to share the video data to the cloud computation server, where the user also ends up sharing the private visual information like gender, skin color, clothing, background objects etc. Therefore, there is a pressing need for solutions to privacy preserving action recognition. Beyond privacy protection, bias in video understanding can lead to unfair and incorrect decision making. Action recognition models may predict specific actions based on gender stereotypes, such as associating a perceived female subject with hands near her face as applying makeup or brushing hair, even with nothing in hand, or they may suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance). In this talk, I will present our recent work on Privacy Preservation and Bias Mitigation in human action recognition.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Dr. Mubarak Shah, the UCF Trustee Chair Professor, is the founding director of Center for Research in Computer Visions at University of Central Florida (UCF). Dr. Shah is a fellow of ACM, IEEE, AAAS, NAI, IAPR, AAIA and SPIE.  He has published extensively on topics related to human activity and action recognition, visual tracking, geo localization, visual crowd analysis, object detection and categorization, shape from shading, etc. He has served as ACM and IEEE Distinguished Visitor Program speaker. He is a recipient of 2022 PAMI Mark Everingham Prize for pioneering human action recognition datasets; 2019 ACM SIGMM Technical Achievement award; 2020 ACM SIGMM Test of Time Honorable Mention Award for his paper “Visual attention detection in video sequences using spatiotemporal cues”; 2020 International Conference on Pattern Recognition (ICPR) Best Scientific Paper Award; an honorable mention for the ICCV 2005 Where Am I? Challenge Problem; 2013 NGA Best Research Poster Presentation; 2nd place in Grand Challenge at the ACM Multimedia 2013 conference; and runner up for the best paper award in ACM Multimedia Conference in 2005 and 2010. At UCF he has received Pegasus Professor Award; University Distinguished Research Award; Faculty Excellence in Mentoring Doctoral Students; Faculty Excellence in Mentoring Postdoctoral Scholars, Scholarship of Teaching and Learning award; Teaching Incentive Program award; and Research Incentive Award.</p>
                </div>
            </div>
        </div><br>
        <!-- 2nd keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Margarita_Chli.png" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://scholar.google.ch/citations?user=C0UhwEIAAAAJ&hl=en">Margarita Chli</a></h4>
                    <span class=""><small>University of Cyprus and ETH Zurich</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b></b></h5>
                    <p class="text-center mb-1"><small></small></p>
                    <p class="pb-1 mb-1 text-justify">{{ person.abstract }}</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Margarita Chli is a Professor of Robotic Vision and the director of the Vision for Robotics Lab, at the University of Cyprus and ETH Zurich. Her work has contributed to the first vision-based autonomous flight of a small drone and the first demonstration of collaborative monocular SLAM for a small swarm of drones. Margarita has given invited keynotes at the World Economic Forum in Davos, TEDx, and ICRA, and she was featured in Robohub's 2016 list of "25 women in Robotics you need to know about". In 2023 she won the ERC Consolidator Grant, one of the most prestigious grants in Europe for blue-sky research, to grow her team at the University of Cyprus to research advanced robotic perception.</p>
                </div>
            </div>
        </div><br>
        <!-- 3rd keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Federico_Tombari.png" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://federicotombari.github.io/">Federico Tombari</a></h4>
                    <span class=""><small>Google</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>The 3D Revolution: Neural Representations and Diffusion Models to Understand and Synthesise the 3D World</b></h5>
                    <p class="text-center mb-1"><small></small></p><br>
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>3D Computer Vision has recently witnessed a surge of interest from the ML and CV research community, due to the progress that recently introduced concepts such as neural representations, foundational models, and diffusion models enabled for many traditional 3D Computer Vision tasks. In this talk, we will focus in particular on the capability of understanding and synthesising 3D scenes and objects, which is a key component of applications in the space of Augmented/Mixed Reality and Robotics. We will look at three tasks in 3D Computer Vision that are fundamental components for these applications while being highly influenced by the aforementioned concepts: novel view synthesis, 3D semantic segmentation and 3D asset generation. For each of these three tasks, we will first understand some important practical limitations of current approaches. We will then walk through some solutions I recently explored with my team and designed to overcome such limitations, which include robust novel view synthesis, open set 3D scene segmentation and realistic 3D asset generation.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Federico Tombari is Senior Staff Research Scientist and Manager at Google where he leads an applied research team in computer vision and machine learning across North America and Europe. He is also a Lecturer (PrivatDozent) at the Technical University of Munich (TUM). He has 250+ peer-reviewed publications in CV/ML and applications to robotics, autonomous driving, healthcare and augmented reality. He got his PhD from the University of Bologna and his Venia Legendi (Habilitation) from Technical University of Munich (TUM). In 2018-19 he was co-founder and managing director of a startup on 3D perception for AR and robotics, then acquired by Google. He regularly serves as Area Chair and Associate Editor for international conferences and journals (IJRR, RA-L, IROS20/21/22, ICRA20/22, 3DV19/20/21/22/24, ECCV22/24, CVPR23/24, NeurIPS23 among others). He was the recipient of two Google Faculty Research Awards, one Amazon Research Award, 5 Outstanding Reviewer Awards (3x CVPR, ICCV21, NeuriIps21), among others. He has been a research partner of private and academic institutions including Google, Toyota, BMW, Audi, Amazon, Univ. Stanford, ETH and MIT.</p>
                </div>
            </div>
        </div>
        <!-- 4th keynote speaker -->
        <!--    commented out by Edmond
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/phil_torr.jpg" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a></h4>
                    <span class=""><small>University of Oxford</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b></b></h5>
                    <p class="text-center mb-1"><small></small></p>
                    <p class="pb-1 mb-1">{{ person.abstract }}</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Professor Philip Torr did his PhD (DPhil) at the Robotics Research Group of the University of Oxford under Professor David Murray of the Active Vision Group. He worked for another three years at Oxford as a research fellow, and still maintains close contact as visiting fellow there. He left Oxford to work for six years as a research scientist for Microsoft Research, first in Redmond, USA, in the Vision Technology Group, then in Cambridge founding the vision side of the Machine Learning and Perception Group. He then became a Professor in in Computer Vision and Machine Learning at Oxford Brookes University, where he has brought in over one million pounds in grants for which he is PI. Recently in 2013, Philip returned to Oxford as full professor where he has established the Torr Vision group. He won several awards including the Marr prize (the highest honour in vision) in 1998. He is a Royal Society Wolfson Research Merit Award Holder. Recently, together with members of his group, he has won several other awards including an honorary mention at the NIPS 2007 conference for the paper 'P. Kumar, V. Kolmorgorov, and P.H.S. Torr, An Analysis of Convex Relaxations for MAP Estimation', in NIPS 21, Neural Information Processing Conference, and (oral) Best Paper at Conference for 'O. Woodford, P.H.S. Torr, I. Reid, and A.W. Fitzgibbon, Global Stereo Reconstruction under Second Order Smoothness Priors', in Proceedings IEEE Conference of Computer Vision and Pattern Recognition, 2008. More recently he has been awarded best science paper at BMVC 2010 and ECCV 2010. He was involved in the algorithm design for Boujou released by 2D3. Boujou has won a clutch of industry awards, including Computer Graphics World Innovation Award, IABM Peter Wayne Award, and CATS Award for Innovation, and a technical EMMY. He then worked closely with this Oxford based company as well as other companies such as Sony on the Wonderbook project. He is a director of new Oxford based spin out OxSight, and Chief Scientific Advisor for Five AI. He was elected Fellow of the Royal Academy of Engineering (FREng) in 2019, and Fellow of the Royal Society (FRS) in 2021 for contributions to computer vision. In 2021 he was made Turing AI world leading researcher fellow.</p>
                </div>
            </div>
        </div>
        //-->
    </div>
</div>
