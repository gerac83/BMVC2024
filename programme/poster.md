---
layout: default_sparse
title: Poster Presentations
permalink: /programme/poster/
index: 12
---

<style id="poster-12_17171_Styles">
<!--table
	{mso-displayed-decimal-separator:"\.";
	mso-displayed-thousand-separator:"\,";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
.style0
	{mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	white-space:nowrap;
	mso-rotate:0;
	mso-background-source:auto;
	mso-pattern:auto;
	color:black;
	font-size:10.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Arial;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	border:none;
	mso-protection:locked visible;
	mso-style-name:Normal;
	mso-style-id:0;}
td
	{mso-style-parent:style0;
	padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:10.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Arial;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{mso-style-parent:style0;
	color:white;
	font-size:11.0pt;
	font-weight:700;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:left;
	border:.5pt solid black;
	background:black;
	mso-pattern:black none;}
.xl66
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border:.5pt solid black;}
.xl67
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	border:.5pt solid black;}
.xl68
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl69
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	border:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl70
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:.5pt solid black;
	border-right:.5pt solid black;
	border-bottom:.5pt solid black;
	border-left:none;}
.xl71
	{mso-style-parent:style0;
	color:windowtext;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:none;}
.xl72
	{mso-style-parent:style0;
	color:windowtext;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:.5pt solid black;
	border-left:none;}
.xl73
	{mso-style-parent:style0;
	color:windowtext;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl74
	{mso-style-parent:style0;
	color:windowtext;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:.5pt solid black;
	border-left:.5pt solid black;}
.xl75
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:center;
	vertical-align:middle;
	border-top:.5pt solid black;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl76
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:center;
	vertical-align:middle;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:none;}
-->
</style>

<div id="poster-12_17171" align=center x:publishsource="Excel">

<table border=0 cellpadding=0 cellspacing=0 width=4387 style='border-collapse:
 collapse;table-layout:fixed;width:3291pt'>
 <col width=101 style='width:76pt'>
 <col width=61 span=2 style='mso-width-source:userset;mso-width-alt:1962;
 width:46pt'>
 <col width=897 style='mso-width-source:userset;mso-width-alt:28714;width:673pt'>
 <col width=2819 style='mso-width-source:userset;mso-width-alt:90197;
 width:2114pt'>
 <col width=448 style='mso-width-source:userset;mso-width-alt:14336;width:336pt'>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl65 width=101 style='height:15.75pt;width:76pt'>Day</td>
  <td class=xl65 width=61 style='border-left:none;width:46pt'>Number</td>
  <td class=xl65 width=61 style='border-left:none;width:46pt'>ID</td>
  <td class=xl65 width=897 style='border-left:none;width:673pt'>Title</td>
  <td class=xl65 width=2819 style='border-left:none;width:2114pt'>Authors</td>
  <td class=xl65 width=448 style='border-left:none;width:336pt'>Topic</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=65 height=1300 class=xl75 style='border-bottom:.5pt solid black;
  height:1023.75pt;border-top:none'>Tuesday</td>
  <td class=xl68 style='border-top:none;border-left:none'>1</td>
  <td class=xl68 style='border-top:none;border-left:none'>194</td>
  <td class=xl69 style='border-top:none;border-left:none'>mmPoint: Dense Human
  Point Cloud Generation from mmWave</td>
  <td class=xl69 style='border-top:none;border-left:none'>Qian Xie (University
  of Oxford)*; Qianyi Deng (University of Oxford); Ta-Ying Cheng (University of
  Oxford); Peijun Zhao (Massachusetts Institute of Technology); Amir Patel
  (University of Cape Town); Niki Trigoni (University of Oxford); Andrew
  Markham (University of Oxford)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from a single
  image and shape-from-x</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>2</td>
  <td class=xl68 style='border-top:none;border-left:none'>356</td>
  <td class=xl69 style='border-top:none;border-left:none'>Lightweight
  Self-Supervised Depth Estimation with few-beams LiDAR Data</td>
  <td class=xl69 style='border-top:none;border-left:none'>Rizhao Fan
  (University of Bologna)*; Fabio Tosi (University of Bologna); Matteo Poggi
  (University of Bologna); Stefano Mattoccia (University of Bologna)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from a single
  image and shape-from-x</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>3</td>
  <td class=xl68 style='border-top:none;border-left:none'>174</td>
  <td class=xl69 style='border-top:none;border-left:none'>Sparse Multi-Object
  Render-and-Compare</td>
  <td class=xl69 style='border-top:none;border-left:none'>Florian Maximilian
  Langer (Department of Engineering, University of Cambridge)*; Ignas Budvytis
  (Department of Engineering, University of Cambridge); Roberto Cipolla
  (University of Cambridge)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from a single
  image and shape-from-x</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>4</td>
  <td class=xl68 style='border-top:none;border-left:none'>90</td>
  <td class=xl69 style='border-top:none;border-left:none'>Floorplan Restoration
  by Structure Hallucinating Transformer Cascades</td>
  <td class=xl69 style='border-top:none;border-left:none'>Sepidehsadat Hosseini
  (Simon Fraser University)*; Yasutaka Furukawa (Simon Fraser University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>5</td>
  <td class=xl68 style='border-top:none;border-left:none'>89</td>
  <td class=xl69 style='border-top:none;border-left:none'>Strong Stereo
  Features for Self-Supervised Practical Stereo Matching</td>
  <td class=xl69 style='border-top:none;border-left:none'>Pierre-André
  Brousseau (Université de Montréal)*; Sebastien Roy (Universite de Montreal)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>6</td>
  <td class=xl68 style='border-top:none;border-left:none'>501</td>
  <td class=xl69 style='border-top:none;border-left:none'>Temporal Lidar Depth
  Completion</td>
  <td class=xl69 style='border-top:none;border-left:none'>Pietari Kaskela
  (NVIDIA)*; Philipp Fischer (NVIDIA); Timo Roman (NVIDIA)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>7</td>
  <td class=xl68 style='border-top:none;border-left:none'>15</td>
  <td class=xl69 style='border-top:none;border-left:none'>The Interstate-24 3D
  Dataset: a new benchmark for 3D multi-camera vehicle tracking</td>
  <td class=xl69 style='border-top:none;border-left:none'>Derek Gloudemans
  (Vanderbilt University)*; Daniel Work (Vanderbilt University); Yanbing Wang
  (Vanderbilt University); Gracie E Gumm (Vanderbilt University); William
  Barbour (Vanderbilt University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>8</td>
  <td class=xl68 style='border-top:none;border-left:none'>448</td>
  <td class=xl69 style='border-top:none;border-left:none'>Optimal Camera
  Configuration for Large-Scale Motion Capture Systems</td>
  <td class=xl69 style='border-top:none;border-left:none'>Xiongming Dai
  (louisiana state university)*; Gerald Baumgartner (Louisiana State
  University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>9</td>
  <td class=xl68 style='border-top:none;border-left:none'>682</td>
  <td class=xl69 style='border-top:none;border-left:none'>ManifoldNeRF:
  View-dependent Image Feature Supervision for Few-shot Neural Radiance Fields</td>
  <td class=xl69 style='border-top:none;border-left:none'>Daiju Kanaoka (Kyushu
  Institute of Technology)*; Motoharu Sonogashira (RIKEN); Hakaru Tamukoh
  (Kyushu Institute of Technology); Yasutomo Kawanishi (RIKEN)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>10</td>
  <td class=xl68 style='border-top:none;border-left:none'>741</td>
  <td class=xl69 style='border-top:none;border-left:none'>Motion-Bias-Free
  Feature-Based SLAM</td>
  <td class=xl69 style='border-top:none;border-left:none'>Alejandro Fontan
  (Queensland University of Technology)*; Michael Milford (ACRV and QUT,
  Australia); Javier Civera (Universidad de Zaragoza)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>11</td>
  <td class=xl68 style='border-top:none;border-left:none'>825</td>
  <td class=xl69 style='border-top:none;border-left:none'>RoomNeRF:
  Representing Empty Room as Neural Radiance Fields for View Synthesis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mangyu Kong (Yonsei
  University)*; Seongwon Lee (Yonsei university); Euntai Kim (Yonsei
  University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from multi-view
  and sensors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>12</td>
  <td class=xl68 style='border-top:none;border-left:none'>304</td>
  <td class=xl69 style='border-top:none;border-left:none'>Learning Part Motion
  of Articulated Objects Using Spatially Continuous Neural Implicit
  Representations</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yushi Du (Peking
  University)*; Ruihai Wu (Peking University); Yan Shen (Peking University);
  Hao Dong (Peking University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Shape modeling and
  processing</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>13</td>
  <td class=xl68 style='border-top:none;border-left:none'>231</td>
  <td class=xl69 style='border-top:none;border-left:none'>Propose-and-Complete:
  Auto-regressive Semantic Group Generation for Personalized Scene Synthesis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Shoulong Zhang
  (Beihang University); Shuai Li (BeihangUniversity); Xinwei Huang (Beihang
  University); Wenchong Xu (Beihang University); Aimin Hao (BeihangUniversity);
  HONG QIN (Stony Brook University)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Shape modeling and
  processing</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>14</td>
  <td class=xl68 style='border-top:none;border-left:none'>306</td>
  <td class=xl69 style='border-top:none;border-left:none'>Point Cloud Sampling
  Preserving Local Geometry for Surface Reconstruction</td>
  <td class=xl69 style='border-top:none;border-left:none'>Kohei Matsuzaki (KDDI
  Research, Inc.)*; Keisuke Nonaka (KDDI Research, Inc.)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Shape modeling and
  processing</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>15</td>
  <td class=xl68 style='border-top:none;border-left:none'>417</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deformation-Guided
  Unsupervised Non-Rigid Shape Matching</td>
  <td class=xl69 style='border-top:none;border-left:none'>Aymen Merrouche
  (INRIA)*; Joao Pedro Cova Regateiro (Interdigital); Stefanie Wuhrer (Inria);
  Edmond Boyer (Inria)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Shape modeling and
  processing</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>16</td>
  <td class=xl68 style='border-top:none;border-left:none'>820</td>
  <td class=xl69 style='border-top:none;border-left:none'>Proposal-based
  Temporal Action Localization with Point-level Supervision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yuan Yin (Institute
  of Industrial Science, The University of Tokyo)*; Yifei Huang (The University
  of Tokyo); Ryosuke Furuta (The University of Tokyo); Yoichi Sato (University
  of Tokyo)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>17</td>
  <td class=xl68 style='border-top:none;border-left:none'>335</td>
  <td class=xl69 style='border-top:none;border-left:none'>Supervised
  Contrastive Learning with Identity-Label Embeddings for Facial Action Unit
  Recognition</td>
  <td class=xl69 style='border-top:none;border-left:none'>Tangzheng Lian
  (Nottingham Trent Univeristy); David A Adama (Nottingham Trent University);
  Pedro Machado (Nottingham Trent University); Doratha E Vinkemeier (NTU)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>18</td>
  <td class=xl68 style='border-top:none;border-left:none'>332</td>
  <td class=xl69 style='border-top:none;border-left:none'>Learning Temporal
  Sentence Grounding From Narrated EgoVideos</td>
  <td class=xl69 style='border-top:none;border-left:none'>Kevin Flanagan
  (University of Bristol)*; Dima Damen (University of Bristol); Michael Wray
  (University of Bristol)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>19</td>
  <td class=xl68 style='border-top:none;border-left:none'>739</td>
  <td class=xl69 style='border-top:none;border-left:none'>Robust Principles:
  Architectural Design Principles for Adversarially Robust CNNs</td>
  <td class=xl69 style='border-top:none;border-left:none'>ShengYun Peng
  (Georgia Institute of Technology)*; Weilin Xu (Intel); Cory Cornelius (Intel
  Corporation); Matthew Hull (Georgia Institute of Technology); Kevin Li
  (Georgia Institute of Technology); Rahul Duggal (Georgia Tech); Mansi Phute
  (Georgia Institute of Technology); Jason Martin (Intel Corporation); Duen
  Horng Chau (Georgia Institute of Technology)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>20</td>
  <td class=xl68 style='border-top:none;border-left:none'>172</td>
  <td class=xl69 style='border-top:none;border-left:none'>Backdoor Attack on
  Hash-based Image Retrieval via Clean-label Data Poisoning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Kuofeng Gao (Tsinghua
  University)*; Jiawang Bai (Tsinghua University); Bin Chen (Harbin Institute
  of Technology, Shenzhen); Dongxian Wu (the University of Tokyo); Shu-Tao Xia
  (Tsinghua University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>21</td>
  <td class=xl68 style='border-top:none;border-left:none'>406</td>
  <td class=xl69 style='border-top:none;border-left:none'>Exploring
  Non-additive Randomness on ViT against Query-Based Black-Box Attacks</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jindong Gu
  (University of Oxford)*; Fangyun Wei (Microsoft Research Asia); Philip Torr
  (University of Oxford); Han Hu (Microsoft Research Asia)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>22</td>
  <td class=xl68 style='border-top:none;border-left:none'>271</td>
  <td class=xl69 style='border-top:none;border-left:none'>Semantic Adversarial
  Attacks via Diffusion Models</td>
  <td class=xl69 style='border-top:none;border-left:none'>Chenan Wang (Drexel
  University)*; Jinhao Duan (Drexel University); Chaowei Xiao (ASU); Edward Kim
  (Drexel University); Matthew c Stamm (Drexel University); Kaidi Xu (Drexel
  University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>23</td>
  <td class=xl68 style='border-top:none;border-left:none'>296</td>
  <td class=xl69 style='border-top:none;border-left:none'>RBFormer: Robust Bias
  Can Improve the Adversarial Robust of Transformer-based Structure</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hao Cheng (The Hong
  Kong University of Science and Technology(Guangzhou))*; Jinhao Duan (Drexel
  University); Hui Li (Samsung Research and Development Institute China Xi'an);
  Lyutianyang Zhang (University of Washington); Jiahang Cao (The Hong Kong
  University of Science and Technology (Guangzhou)); Ping Wang (Xi'an Jiaotong
  University); Jize Zhang (HKUST); Kaidi Xu (Drexel University); Renjing Xu
  (The Hong Kong University of Science and Technology (Guangzhou))</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>24</td>
  <td class=xl68 style='border-top:none;border-left:none'>486</td>
  <td class=xl69 style='border-top:none;border-left:none'>ADoPT: LiDAR Spoofing
  Attack Detection based on Point-Level Temporal Consistency</td>
  <td class=xl69 style='border-top:none;border-left:none'>Minkyoung Cho
  (University of Michigan)*; Yulong Cao (Nvidia); Zixiang Zhou (University of
  Michigan); Zhuoqing Morley Mao (University of Michigan)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>25</td>
  <td class=xl68 style='border-top:none;border-left:none'>620</td>
  <td class=xl69 style='border-top:none;border-left:none'>Unifying the Harmonic
  Analysis of Adversarial Attacks and Robustness</td>
  <td class=xl69 style='border-top:none;border-left:none'>Shishira R R Maiya
  (University of Maryland)*; Max Ehrlich (NVIDIA); Vatsal Agarwal (University
  of Maryland); Ser-Nam Lim (Meta AI); Tom Goldstein (University of Maryland);
  Abhinav Shrivastava (University of Maryland)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>26</td>
  <td class=xl68 style='border-top:none;border-left:none'>781</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adaptive Adversarial
  Norm Space for Efficient Adversarial Training</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hui Kuurila-Zhang
  (University of Oulu)*; Haoyu Chen (University of Oulu); Guoying Zhao
  (University of Oulu)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adversarial attack
  and defense</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>27</td>
  <td class=xl68 style='border-top:none;border-left:none'>382</td>
  <td class=xl69 style='border-top:none;border-left:none'>Fully Quantum
  Auto-Encoding of 3D Shapes</td>
  <td class=xl69 style='border-top:none;border-left:none'>Lakshika Rathi
  (Indian Institute of Technology Delhi); Edith Tretschk (Max-Planck-Institut
  für Informatik)*; Christian Theobalt (MPI Informatik); Rishabh Dabral (IIT
  Bombay); Vladislav Golyanik (MPI for Informatics)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Brave new ideas</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>28</td>
  <td class=xl68 style='border-top:none;border-left:none'>677</td>
  <td class=xl69 style='border-top:none;border-left:none'>MFSC: Matching by
  Few-Shot Classification</td>
  <td class=xl69 style='border-top:none;border-left:none'>Daniel Shalam
  (University of Haifa); Elie Abboud (University of Haifa); Roee Litman (-);
  Simon Korman (University of Haifa)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Brave new ideas</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>29</td>
  <td class=xl68 style='border-top:none;border-left:none'>822</td>
  <td class=xl69 style='border-top:none;border-left:none'>Differentiable SLAM
  Helps Deep Learning-based LiDAR Perception Tasks</td>
  <td class=xl69 style='border-top:none;border-left:none'>Prashant Kumar
  (Indian Institute of Technology, Delhi)*; Dheeraj Vattikonda (McGill
  University); Vedang Bhupesh Shenvi Nadkarni (Birla Institute of Technology
  and Science, Pilani); Erqun Dong (McGill University); Sabyasachi Sahoo
  (Université Laval, Mila)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Brave new ideas</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>30</td>
  <td class=xl68 style='border-top:none;border-left:none'>643</td>
  <td class=xl69 style='border-top:none;border-left:none'>Color Constancy: How
  to Deal with Camera Bias?</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yi-Tun Lin
  (University of East Anglia)*; Bianjiang Yang (Purdue University); Hao Xie
  (Meta Platforms, Inc.); Wenbin Wang (Meta); Honghong Peng (Meta); JUN HU
  (Apple Inc)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Computational
  Photography</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>31</td>
  <td class=xl68 style='border-top:none;border-left:none'>348</td>
  <td class=xl69 style='border-top:none;border-left:none'>RGB and LUT based
  Cross Attention Network for Image Enhancement</td>
  <td class=xl69 style='border-top:none;border-left:none'>Tengfei Shi (Beihang
  University); Chenglizhao Chen (China University of Petroleum (East China))*;
  Yuanbo He (State Key Laboratory of Virtual Reality Technology and Systems,
  Beihang University); wenfeng song (Beijing Information Science and Technology
  University); Aimin Hao (BeihangUniversity)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Computational
  Photography</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>32</td>
  <td class=xl68 style='border-top:none;border-left:none'>315</td>
  <td class=xl69 style='border-top:none;border-left:none'>Generalized Imaging
  Augmentation via Linear Optimization of Neurons</td>
  <td class=xl69 style='border-top:none;border-left:none'>Daoyu Li (Beijing
  Institute of Technology); Lu Li (Beijing Institute of Technology); Bin Li
  (Beijing University of Posts and Telecommunications); Liheng Bian (Beijing
  Institute of Technology)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Computational
  Photography</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>33</td>
  <td class=xl68 style='border-top:none;border-left:none'>765</td>
  <td class=xl69 style='border-top:none;border-left:none'>Reconstructing
  Synthetic Lensless Images in the Low-Data Regime</td>
  <td class=xl69 style='border-top:none;border-left:none'>Abeer Banerjee
  (CSIR-CEERI)*; Himanshu Kumar (CSIR-CEERI); Sumeet Saurav (CSIR-CEERI);
  Sanjay Singh (CSIR-CEERI, Pilani )</td>
  <td class=xl69 style='border-top:none;border-left:none'>Computational
  Photography</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>34</td>
  <td class=xl68 style='border-top:none;border-left:none'>286</td>
  <td class=xl69 style='border-top:none;border-left:none'>Lightweight Image
  Super-Resolution with Scale-wise Network</td>
  <td class=xl69 style='border-top:none;border-left:none'>Xiaole Zhao (School
  of Computing and Artificial Intelligence, Southwest Jiaotong University);
  Xinkun Wu (School of Computing and Artificial Intelligence, Southwest
  Jiaotong University)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Computer vision
  theory</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>35</td>
  <td class=xl68 style='border-top:none;border-left:none'>540</td>
  <td class=xl69 style='border-top:none;border-left:none'>Sketch-based Video
  Object Segmentation: Benchmark and Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ruolin Yang (Beijing
  University of Posts and Telecommunications)*; Da Li (Samsung); Conghui Hu
  (National University of Singapore); Timothy Hospedales (Edinburgh
  University); Honggang Zhang (Beijing University of Posts and
  Telecommunications); Yi-Zhe Song (University of Surrey)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Datasets and
  Evaluation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>36</td>
  <td class=xl68 style='border-top:none;border-left:none'>870</td>
  <td class=xl69 style='border-top:none;border-left:none'>Data exploitation:
  multi-task learning of object detection and semantic segmentation on
  partially annotated data</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hoàng-Ân Lê (IRISA,
  University of South Brittany)*; Minh-Tan Pham (IRISA-UBS)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Datasets and
  Evaluation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>37</td>
  <td class=xl68 style='border-top:none;border-left:none'>235</td>
  <td class=xl69 style='border-top:none;border-left:none'>What Should be
  Balanced in a “Balanced” Dataset?</td>
  <td class=xl69 style='border-top:none;border-left:none'>Haiyu Wu (University
  of Notre Dame)*; Kevin Bowyer (University of Notre Dame)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Datasets and
  Evaluation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>38</td>
  <td class=xl68 style='border-top:none;border-left:none'>127</td>
  <td class=xl69 style='border-top:none;border-left:none'>SynthBlink and
  BlinkFormer: A Synthetic Dataset and Transformer-Based Method for Video Blink
  Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Bo Liu (Beihang
  University); Yang Xu (Beihang University); Feng Lu (Beihang University)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Datasets and
  Evaluation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>39</td>
  <td class=xl68 style='border-top:none;border-left:none'>743</td>
  <td class=xl69 style='border-top:none;border-left:none'>A Comprehensive
  Crossroad Camera Dataset to Improve Traffic Safety of Mobility Aid Users</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ludwig Mohr
  (Institute of Computer Graphics and Vision, Graz University of Technology)*;
  Nadezda Kirillova (Graz University of Technology); Horst Possegger (Graz
  University of Technology); Horst Bischof (Graz University of Technology)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Datasets and
  Evaluation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>40</td>
  <td class=xl68 style='border-top:none;border-left:none'>752</td>
  <td class=xl69 style='border-top:none;border-left:none'>Learnable Data
  Augmentation for One-Shot Unsupervised Domain Adaptation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Julio Ivan Davila
  Carrazco (Istituto Italiano di Tecnologia)*; Pietro Morerio (Istituto
  Italiano di Tecnologia); Alessio Del Bue (Istituto Italiano di Tecnologia
  (IIT)); Vittorio Murino (Istituto Italiano di Tecnologia)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>41</td>
  <td class=xl68 style='border-top:none;border-left:none'>660</td>
  <td class=xl69 style='border-top:none;border-left:none'>G2N2: Lightweight
  Event Stream Classification with GRU Graph Neural Networks</td>
  <td class=xl69 style='border-top:none;border-left:none'>Thomas Mesquida (CEA
  LIST)*; Manon Dampfhoffer (SPINTEC University Grenoble Alpes); Thomas Dalgaty
  (CEA List); Pascal Vivet (CEA-LIST); Amos Sironi (PROPHESEE); Christoph Posch
  (PROPHESEE)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>42</td>
  <td class=xl68 style='border-top:none;border-left:none'>709</td>
  <td class=xl69 style='border-top:none;border-left:none'>Momentum Adapt:
  Robust Unsupervised Adaptation for Improving Temporal Consistency in Video
  Semantic Segmentation During Test-Time</td>
  <td class=xl69 style='border-top:none;border-left:none'>Amirhossein
  Hassankhani (Tampere University)*; Hamed Rezazadegan Tavakoli (Nokia
  Technologies); Esa Rahtu (Tampere University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>43</td>
  <td class=xl68 style='border-top:none;border-left:none'>123</td>
  <td class=xl69 style='border-top:none;border-left:none'>Knowledge
  Distillation Layer that Lets the Student Decide</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ada Gorgun (Middle
  East Technical University)*; Yeti Z. Gurbuz (Tecnische Universitat Berlin);
  Aydin Alatan (Middle East Technical University, Turkey)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>44</td>
  <td class=xl68 style='border-top:none;border-left:none'>237</td>
  <td class=xl69 style='border-top:none;border-left:none'>Spatio-Temporal
  MLP-Graph Network for 3D Human Pose Estimation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Md. Tanvir Hassan
  (Concordia University); Abdessamad Ben Hamza (Concordia University)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>45</td>
  <td class=xl68 style='border-top:none;border-left:none'>360</td>
  <td class=xl69 style='border-top:none;border-left:none'>FLRKD: Relational
  Knowledge Distillation Based on Channel-wise Feature Quality Assessment</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zeyu An (University
  of Electronic Science and Technology of China)*; Changjian Deng (University
  of Electronic Science and Technology of China); Wanli Dang (University of
  Electronic Science and Technology of China;The Second Research Institute of
  the Civil Aviation Administration of China); Zhicheng Dong (Tibet
  university); 谦 罗 (中国民用航空总局第二研究所); Jian Cheng (University of Electronic
  Science and Technology of China)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>46</td>
  <td class=xl68 style='border-top:none;border-left:none'>792</td>
  <td class=xl69 style='border-top:none;border-left:none'>Budding Ensemble
  Architecture: Revisiting anchor-based object detection DNN</td>
  <td class=xl69 style='border-top:none;border-left:none'>Qutub Syed (INTEL
  LABS)*; Neslihan Kose Cihangir (Intel Deutschland GmbH); Rafael Rosales
  (Intel); Michael Paulitsch (Intel); Korbinian Hagn (Intel); Florian R
  Geissler (Intel); Yang Peng (Intel); Gereon Hinz (STTech GmbH); Alois C.
  Knoll (Robotics and Embedded Systems)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>47</td>
  <td class=xl68 style='border-top:none;border-left:none'>893</td>
  <td class=xl69 style='border-top:none;border-left:none'>VADOR: Real World
  Video Anomaly Detection with Object Relations and Action</td>
  <td class=xl69 style='border-top:none;border-left:none'>Halil İbrahim Öztürk
  (Togg)*; Ahmet Burak Can (Hacettepe University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>48</td>
  <td class=xl68 style='border-top:none;border-left:none'>911</td>
  <td class=xl69 style='border-top:none;border-left:none'>Masked Attention
  ConvNeXt Unet with Multi-Synthetic Dynamic Weighting for Anomaly Detection
  and Localization</td>
  <td class=xl69 style='border-top:none;border-left:none'>SHIH CHIH LIN
  (National Tsing Hua University)*; Ho Weng Lee (National Tsing Hua
  University); Yu-Shuan Hsieh (National Tsing Hua University); Cheng Yu Ho
  (National Tsing Hua University); Shang-Hong Lai (National Tsing Hua
  University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>49</td>
  <td class=xl68 style='border-top:none;border-left:none'>204</td>
  <td class=xl69 style='border-top:none;border-left:none'>Cardiac Landmark
  Detection using Generative Adversarial Networks from Cardiac MR Images</td>
  <td class=xl69 style='border-top:none;border-left:none'>Aparna Kanakatte
  (TCS)*; DIVYA M BHATIA (TCS); Pavan Kumar Reddy K (TCS Research);
  Jayavardhana Gubbi (TCS Research); Avik Ghose (TCS)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>50</td>
  <td class=xl68 style='border-top:none;border-left:none'>514</td>
  <td class=xl69 style='border-top:none;border-left:none'>DFFG: Fast Gradient
  Iteration for Data-free Quantization</td>
  <td class=xl69 style='border-top:none;border-left:none'>huixing leng (Beihang
  University); shuangkang fang (megvii,buaa); Yufeng Wang (Beihang
  University)*; Zehao ZHANG (beihang university); Qi Dacheng (Beijing Jiaotong
  University); Wenrui Ding (Beihang University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>51</td>
  <td class=xl68 style='border-top:none;border-left:none'>522</td>
  <td class=xl69 style='border-top:none;border-left:none'>Train ViT on Small
  Dataset With Translation Perceptibility</td>
  <td class=xl69 style='border-top:none;border-left:none'>CHEN HUAN (Institute
  of Computing Technology)*; Ping Yao (Institute of Computing Technology,
  Chinese Academy of Sciences ); WENTAO WEI (Southeast University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>52</td>
  <td class=xl68 style='border-top:none;border-left:none'>665</td>
  <td class=xl69 style='border-top:none;border-left:none'>Distillation for
  High-Quality Knowledge Extraction via Explainable Oracle Approach</td>
  <td class=xl69 style='border-top:none;border-left:none'>MyungHak Lee (Kookmin
  University)*; Wooseong Syz Cho (Kookmin University); Sungsik Kim (Kookmin
  University); Jinkyu Kim (Korea University); Jaekoo Lee (Kookmin University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>53</td>
  <td class=xl68 style='border-top:none;border-left:none'>846</td>
  <td class=xl69 style='border-top:none;border-left:none'>Topology-Preserving
  Hard Pixel Mining for Tubular Structure Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Guoqing Zhang
  (Tsinghua-Berkeley Shenzhen Institute, Tsinghua University)*; Caixia Dong
  (The Second Affilated Hospital of Xi'an Jiaotong University); Yang Li
  (Tsinghua-Berkeley Shenzhen Institute, Tsinghua University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>54</td>
  <td class=xl68 style='border-top:none;border-left:none'>854</td>
  <td class=xl69 style='border-top:none;border-left:none'>A Forward-backward
  Learning strategy for CNNs via Separation Index Maximizing at the First
  Convolutional Layer</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ali Karimi
  (University of Tehran); Ahmad Kalhor (University of Tehran)*; Mona Ahmadian
  (University of Surrey)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>55</td>
  <td class=xl68 style='border-top:none;border-left:none'>214</td>
  <td class=xl69 style='border-top:none;border-left:none'>Understanding
  Gaussian Attention Bias of Vision Transformers Using Effective Receptive
  Fields</td>
  <td class=xl69 style='border-top:none;border-left:none'>Bum Jun Kim
  (POSTECH); Hyeyeon Choi (POSTECH); Hyeonah Jang (POSTECH); Sang Woo Kim
  (POSTECH)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>56</td>
  <td class=xl68 style='border-top:none;border-left:none'>295</td>
  <td class=xl69 style='border-top:none;border-left:none'>LOCATE:
  Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped
  Self-training</td>
  <td class=xl69 style='border-top:none;border-left:none'>Silky Singh (Adobe
  Systems)*; Shripad V Deshmukh (Adobe); Mausoom Sarkar (Adobe); Balaji
  Krishnamurthy ()</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>57</td>
  <td class=xl68 style='border-top:none;border-left:none'>562</td>
  <td class=xl69 style='border-top:none;border-left:none'>Fiducial Focus
  Augmentation for Facial Landmark Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Purbayan Kar (Sony
  Research India); Vishal M Chudasama (Sony Research India); Naoyuki Onoe
  (Sony); Pankaj Wasnik (Sony Research India)*; Vineeth Balasubramanian (Indian
  Institute of Technology Hyderabad)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>58</td>
  <td class=xl68 style='border-top:none;border-left:none'>268</td>
  <td class=xl69 style='border-top:none;border-left:none'>Lips-SpecFormer:
  Non-Linear Interpolable Transformer for Spectral Reconstruction using
  Adjacent Channel Coupling</td>
  <td class=xl69 style='border-top:none;border-left:none'>Abhishek Kumar Sinha
  (Indian Space Research Organization)*; Manthira Moorthi S (ISRO)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deep learning
  architectures and techniques</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>59</td>
  <td class=xl68 style='border-top:none;border-left:none'>521</td>
  <td class=xl69 style='border-top:none;border-left:none'>Selective Scene Text
  Removal</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hayato Mitani (Kyushu
  University)*; Akisato Kimura (NTT Corporation); Seiichi Uchida (Kyushu
  University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Document analysis and
  understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>60</td>
  <td class=xl68 style='border-top:none;border-left:none'>7</td>
  <td class=xl69 style='border-top:none;border-left:none'>HWD: A Novel
  Evaluation Score for Styled Handwritten Text Generation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vittorio Pippi
  (University of Modena and Reggio Emilia)*; Fabio Quattrini (University of
  Modena and Reggio Emilia); Silvia Cascianelli (Università di Modena e Reggio
  Emilia); Rita Cucchiara (Università di Modena e Reggio Emilia)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Document analysis and
  understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>61</td>
  <td class=xl68 style='border-top:none;border-left:none'>511</td>
  <td class=xl69 style='border-top:none;border-left:none'>McQueen: Mixed
  Precision Quantization of Early Exit Networks</td>
  <td class=xl69 style='border-top:none;border-left:none'>Utkarsh Saxena
  (Purdue University)*; Kaushik Roy (Purdue Uniiversity)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>62</td>
  <td class=xl68 style='border-top:none;border-left:none'>345</td>
  <td class=xl69 style='border-top:none;border-left:none'>Region-aware
  Knowledge Distillation for Efficient Image-to-Image Translation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Linfeng Zhang
  (Tsinghua University )*; Xin Chen (Intel Corp.); Runpei Dong (Xi'an Jiaotong
  University); Kaisheng Ma (Tsinghua University )</td>
  <td class=xl69 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>63</td>
  <td class=xl68 style='border-top:none;border-left:none'>744</td>
  <td class=xl69 style='border-top:none;border-left:none'>CoordGate:
  Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural
  Networks</td>
  <td class=xl69 style='border-top:none;border-left:none'>Sunny Howard
  (University of Oxford)*; Peter Norreys (University of Oxford); Andreas Döpp
  (LMU Munich)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>64</td>
  <td class=xl68 style='border-top:none;border-left:none'>307</td>
  <td class=xl69 style='border-top:none;border-left:none'>RUPQ: Improving
  low-bit quantization by equalizing relative updates of quantization
  parameters</td>
  <td class=xl69 style='border-top:none;border-left:none'>Valentin Buchnev
  (Huawei Technologies Co. Ltd.)*; Jiao He (huawei company); Fengyu Sun
  (Huawei); Ivan Koryakovskiy (Huawei Technologies Co., Ltd.)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>65</td>
  <td class=xl68 style='border-top:none;border-left:none'>889</td>
  <td class=xl69 style='border-top:none;border-left:none'>DeepliteRT: Computer
  Vision at the Edge</td>
  <td class=xl69 style='border-top:none;border-left:none'>Saad Ashfaq
  (Deeplite)*; Alexander Hoffman (McGill University); SAPTARSHI MITRA (Deeplite
  Inc.); Ehsan Saboori (Deeplite Inc.); Sudhakar Sah (Deeplite Inc);
  MohammadHossein AskariHemmat (Polytechnique Montreal)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=66 height=1320 class=xl76 style='border-bottom:.5pt solid black;
  height:1039.5pt'>Wednesday</td>
  <td class=xl70 style='border-top:none'>66</td>
  <td class=xl66 style='border-top:none;border-left:none'>622</td>
  <td class=xl67 style='border-top:none;border-left:none'>Teaching AI to Teach:
  Leveraging Limited Human Salience Data Into Unlimited Saliency-Based Training</td>
  <td class=xl67 style='border-top:none;border-left:none'>Colton R Crum
  (University of Notre Dame)*; Aidan Boyd (University of Notre Dame); Kevin
  Bowyer (University of Notre Dame); Adam Czajka (University of Notre Dame)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Biometrics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>67</td>
  <td class=xl66 style='border-top:none;border-left:none'>498</td>
  <td class=xl67 style='border-top:none;border-left:none'>CERiL: Continuous
  Event-based Reinforcement Learning</td>
  <td class=xl67 style='border-top:none;border-left:none'>Celyn Walters
  (University of Surrey); Simon Hadfield (University of Surrey)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Embodied vision:
  Active agents; simulation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>68</td>
  <td class=xl66 style='border-top:none;border-left:none'>703</td>
  <td class=xl67 style='border-top:none;border-left:none'>Foveation in the Era
  of Deep Learning</td>
  <td class=xl67 style='border-top:none;border-left:none'>Gerardo
  Aragon-Camarasa (University of Glasgow); George W Killick (University of
  Glasgow)*; Paul Henderson (University of Glasgow); Jan Paul Siebert
  (University of Glasgow)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Embodied vision:
  Active agents; simulation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>69</td>
  <td class=xl66 style='border-top:none;border-left:none'>828</td>
  <td class=xl67 style='border-top:none;border-left:none'>SCAAT: Improving
  Neural Network Interpretability via Saliency Constrained Adaptive Adversarial
  Training</td>
  <td class=xl67 style='border-top:none;border-left:none'>Rui Xu (Peking
  University); Wenkang Qin (Peking University); Peixiang Huang (Peking
  University); Hao Wang (National Institutes for Food and Drug Control); Lin
  Luo (Peking University)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Explainable AI</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>70</td>
  <td class=xl66 style='border-top:none;border-left:none'>188</td>
  <td class=xl67 style='border-top:none;border-left:none'>Diverse Explanations
  for Object Detectors with Nesterov-Accelerated iGOS++</td>
  <td class=xl67 style='border-top:none;border-left:none'>Mingqi Jiang (Oregon
  State University)*; Saeed Khorram (Oregon State University); Li Fuxin (Oregon
  State University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Explainable AI</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>71</td>
  <td class=xl66 style='border-top:none;border-left:none'>40</td>
  <td class=xl67 style='border-top:none;border-left:none'>Learning a Pedestrian
  Social Behavior Dictionary</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faith M Johnson
  (Rutgers University)*; Kristin Dana (Rutgers University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Explainable AI</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>72</td>
  <td class=xl66 style='border-top:none;border-left:none'>207</td>
  <td class=xl67 style='border-top:none;border-left:none'>Embedding Human
  Knowledge into Spatio-Temproal Attention Branch Network in Video Recognition
  via Temporal attention</td>
  <td class=xl67 style='border-top:none;border-left:none'>Saki Noguchi (Chubu
  University)*; Yuzhi Shi ( Chubu University); Tsubasa Hirakawa (Chubu
  University); Takayoshi Yamashita (Chubu University); Hironobu Fujiyoshi
  (Chubu University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Explainable AI</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>73</td>
  <td class=xl66 style='border-top:none;border-left:none'>761</td>
  <td class=xl67 style='border-top:none;border-left:none'>Laughing Matters:
  Introducing Audio-Driven Laughing-Face Generation with Diffusion Models</td>
  <td class=xl67 style='border-top:none;border-left:none'>Antoni Bigata
  Casademunt (Imperial College London)*; Rodrigo Mira (Imperial College
  London); Nikita Drobyshev (Imperial College London); Konstantinos Vougioukas
  (Imperial College London); Stavros Petridis (Imperial College London); Maja
  Pantic (Facebook / Imperial College London )</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faces and gestures</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>74</td>
  <td class=xl66 style='border-top:none;border-left:none'>146</td>
  <td class=xl67 style='border-top:none;border-left:none'>Learning Separable
  Hidden Unit Contributions for Speaker-Adaptive Visual Speech Recognition</td>
  <td class=xl67 style='border-top:none;border-left:none'>Songtao Luo
  (Institute of Computing Technology, Chinese Academy of Sciences)*; Shuang
  Yang (ICT, CAS); Shiguang Shan (Institute of Computing Technology, Chinese
  Academy of Sciences); Xilin Chen (Institute of Computing Technology, Chinese
  Academy of Sciences)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faces and gestures</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>75</td>
  <td class=xl66 style='border-top:none;border-left:none'>190</td>
  <td class=xl67 style='border-top:none;border-left:none'>UniLip: Learning
  Visual-Textual Mapping with Uni-Modal Data for Lip Reading</td>
  <td class=xl67 style='border-top:none;border-left:none'>Bingquan Xia
  (Institute of Computing Technology, Chinese Academy of Sciences)*; Shuang
  Yang (ICT, CAS); Shiguang Shan (Institute of Computing Technology, Chinese
  Academy of Sciences); Xilin Chen (Institute of Computing Technology, Chinese
  Academy of Sciences)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faces and gestures</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>76</td>
  <td class=xl66 style='border-top:none;border-left:none'>470</td>
  <td class=xl67 style='border-top:none;border-left:none'>KFC: Kinship
  Verification with Fair Contrastive loss and Multi-Task Learning</td>
  <td class=xl67 style='border-top:none;border-left:none'>Jia Luo Peng
  (National Tsing Hua University)*; Keng Wei Chang (National Tsing Hua
  University); Shang-Hong Lai (National Tsing Hua University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faces and gestures</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>77</td>
  <td class=xl66 style='border-top:none;border-left:none'>98</td>
  <td class=xl67 style='border-top:none;border-left:none'>Prompting
  Visual-Language Models for Dynamic Facial Expression Recognition</td>
  <td class=xl67 style='border-top:none;border-left:none'>Zengqun Zhao (Queen
  Mary University of London)*; Ioannis Patras (Queen Mary University of London)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faces and gestures</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>78</td>
  <td class=xl66 style='border-top:none;border-left:none'>287</td>
  <td class=xl67 style='border-top:none;border-left:none'>EventFormer: AU Event
  Transformer for Facial Action Unit Event Detection</td>
  <td class=xl67 style='border-top:none;border-left:none'>Yingjie Chen (Peking
  University)*; Jiarui Zhang (Peking University); Tao Wang (Peking University);
  Yun Liang (Peking University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Faces and gestures</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>79</td>
  <td class=xl66 style='border-top:none;border-left:none'>230</td>
  <td class=xl67 style='border-top:none;border-left:none'>De-identification of
  facial videos while preserving remote physiological utility</td>
  <td class=xl67 style='border-top:none;border-left:none'>Marko Radisa Savic
  (University of Oulu)*; Guoying Zhao (University of Oulu)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Fairness, privacy,
  ethics, social-good, transparency, accountability in visio<span
  style='display:none'>n</span></td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>80</td>
  <td class=xl66 style='border-top:none;border-left:none'>629</td>
  <td class=xl67 style='border-top:none;border-left:none'>Biased Attention: Do
  Vision Transformers Amplify Gender Bias More than Convolutional Neural
  Networks?</td>
  <td class=xl67 style='border-top:none;border-left:none'>Abhishek Mandal
  (Dublin City University)*; Susan Leavy (University College Dublin); Suzanne
  Little (Dublin City University, Ireland)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Fairness, privacy,
  ethics, social-good, transparency, accountability in visio<span
  style='display:none'>n</span></td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>81</td>
  <td class=xl66 style='border-top:none;border-left:none'>799</td>
  <td class=xl67 style='border-top:none;border-left:none'>Discriminative
  Adversarial Privacy: Balancing Accuracy and Membership Privacy in Neural
  Networks</td>
  <td class=xl67 style='border-top:none;border-left:none'>Eugenio Lomurno
  (Politecnico di Milano)*; Alberto Archetti (Politecnico di Milano); Francesca
  Ausonio (Politecnico di Milano); Matteo Matteucci (Politecnico di Milano)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Fairness, privacy,
  ethics, social-good, transparency, accountability in visio<span
  style='display:none'>n</span></td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>82</td>
  <td class=xl66 style='border-top:none;border-left:none'>143</td>
  <td class=xl67 style='border-top:none;border-left:none'>Sparse and
  Privacy-enhanced Representation for Human Pose Estimation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Ting-Ying Lin
  (National Tsing Hua University)*; Lin-Yung Hsieh (National Tsing Hua
  University); Fu-En Wang (National Tsing Hua University); Wen-Shen Wuen
  (Novatek Microelectronics Corp.); Min Sun (NTHU)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Human pose/shape
  estimation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>83</td>
  <td class=xl66 style='border-top:none;border-left:none'>763</td>
  <td class=xl67 style='border-top:none;border-left:none'>BoIR: Box-Supervised
  Instance Representation for Multi Person Pose Estimation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Uyoung Jeong (Ulsan
  National Institute of Science and Technology)*; Seungryul Baek (UNIST); Hyung
  Jin Chang (University of Birmingham); Kwang In Kim (POSTECH)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Human pose/shape
  estimation</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>84</td>
  <td class=xl66 style='border-top:none;border-left:none'>664</td>
  <td class=xl67 style='border-top:none;border-left:none'>Stream-based Active
  Learning by Exploiting Temporal Properties in Perception with Temporal
  Predicted Loss</td>
  <td class=xl67 style='border-top:none;border-left:none'>Sebastian Schmidt
  (BMW)*; Stephan Günnemann (Technical University of Munich)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Human-in-the-loop
  computer vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>85</td>
  <td class=xl66 style='border-top:none;border-left:none'>798</td>
  <td class=xl67 style='border-top:none;border-left:none'>Cascade Sparse
  Feature Propagation Network for Interactive Segmentation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Chuyu Zhang (PLUS
  Lab, Shanghaitech University)*; Hui Ren (ShanghaiTech); ChuanYang Hu (PLUS
  Lab, Shanghaitech University); Yongfei Liu (ShanghaiTech); Xuming He
  (ShanghaiTech University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Human-in-the-loop
  computer vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>86</td>
  <td class=xl66 style='border-top:none;border-left:none'>337</td>
  <td class=xl67 style='border-top:none;border-left:none'>Active Learning for
  Fine-Grained Sketch-Based Image Retrieval</td>
  <td class=xl67 style='border-top:none;border-left:none'>Himanshu Thakur
  (Carnegie Mellon University); Soumitri Chattopadhyay (Jadavpur University)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Human-in-the-loop
  computer vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>87</td>
  <td class=xl66 style='border-top:none;border-left:none'>718</td>
  <td class=xl67 style='border-top:none;border-left:none'>Self-supervised
  Adversarial Training for Robust Face Forgery Detection</td>
  <td class=xl67 style='border-top:none;border-left:none'>Yueying Gao
  (Communication University of China)*; Weiguo Lin (Communication University of
  China); junfeng xu (Communication University of China); Wanshan Xu
  (Communication University of China); Peibin Chen (Communication University of
  China)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and video
  forensics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>88</td>
  <td class=xl66 style='border-top:none;border-left:none'>379</td>
  <td class=xl67 style='border-top:none;border-left:none'>Test-Time Adaptation
  for Robust Face Anti-Spoofing</td>
  <td class=xl67 style='border-top:none;border-left:none'>Pei-Kai Huang
  (National Tsing Hua University)*; Chen-Yu Lu (National Tsing Hua University);
  Shu-Jung Chang (National Tsing Hua University); Jun-Xiong Chong (National
  Tsing Hua University); Chiou-Ting Hsu (National Tsing Hua University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and video
  forensics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>89</td>
  <td class=xl66 style='border-top:none;border-left:none'>659</td>
  <td class=xl67 style='border-top:none;border-left:none'>Open Set Synthetic
  Image Source Attribution</td>
  <td class=xl67 style='border-top:none;border-left:none'>Shengbang Fang
  (Drexel University)*; Tai D Nguyen (Drexel University); Matthew c Stamm
  (Drexel University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and video
  forensics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>90</td>
  <td class=xl66 style='border-top:none;border-left:none'>595</td>
  <td class=xl67 style='border-top:none;border-left:none'>Face Aging via
  Diffusion-based Editing</td>
  <td class=xl67 style='border-top:none;border-left:none'>Xiangyi Chen (Télécom
  Paris, Shanghai Jiao Tong University)*; Stéphane Lathuilière (Telecom-Paris)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and Video
  Synthesis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>91</td>
  <td class=xl66 style='border-top:none;border-left:none'>236</td>
  <td class=xl67 style='border-top:none;border-left:none'>Temporal-controlled
  Frame Swap for Generating High-Fidelity Stereo Driving Data for Autonomy
  Analysis</td>
  <td class=xl67 style='border-top:none;border-left:none'>Yedi Luo
  (Northeastern University); Xiangyu Bai (Northeastern University); Jiang Le
  (Northeastern University ); Aniket Gupta (Northeastern University); Eric C
  Mortin (US Army DEVCOM Analysis Center); Hanumant Singh (Northeastern
  University); Sarah Ostadabbas (Northeastern University)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and Video
  Synthesis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>92</td>
  <td class=xl66 style='border-top:none;border-left:none'>16</td>
  <td class=xl67 style='border-top:none;border-left:none'>VETIM: Expanding the
  Vocabulary of Text-to-Image Models only with Text</td>
  <td class=xl67 style='border-top:none;border-left:none'>Martin Nicolas
  Everaert (EPFL)*; Radhakrishna Achanta (EPFL); Marco Bocchio (Largo.ai); Sami
  Arpa (Largo.ai); Sabine Süsstrunk (EPFL)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and Video
  Synthesis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>93</td>
  <td class=xl66 style='border-top:none;border-left:none'>258</td>
  <td class=xl67 style='border-top:none;border-left:none'>A Structure-Guided
  Diffusion Model for Large-Hole Image Completion</td>
  <td class=xl67 style='border-top:none;border-left:none'>Daichi Horita (The
  University of Tokyo)*; Jiaolong Yang (Microsoft Research); Dong Chen
  (Microsoft Research Asia); Yuki Koyama (National Institute of Advanced
  Industrial Science and Technology (AIST)); Kiyoharu Aizawa (The University of
  Tokyo); Nicu Sebe (University of Trento)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and Video
  Synthesis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>94</td>
  <td class=xl66 style='border-top:none;border-left:none'>103</td>
  <td class=xl67 style='border-top:none;border-left:none'>Video Infilling with
  Rich Motion Prior</td>
  <td class=xl67 style='border-top:none;border-left:none'>Xinyu Hou (Nanyang
  Technological University)*; Liming Jiang (Nanyang Technological University);
  Rui Shao (Harbin Institute of Technology (Shenzhen)); Chen Change Loy
  (Nanyang Technological University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Image and Video
  Synthesis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>95</td>
  <td class=xl66 style='border-top:none;border-left:none'>274</td>
  <td class=xl67 style='border-top:none;border-left:none'>Frequency-consistent
  Optimization for Image Enhancement Networks</td>
  <td class=xl67 style='border-top:none;border-left:none'>Bing Li (University
  of Science and Technology of China)*; Naishan Zheng (University of Science
  and Technology of China); Qi Zhu (University of Science and Technology of
  China); Jie Huang (University of Science and Technology of China); Feng Zhao
  (University of Science and Technology of China)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>96</td>
  <td class=xl66 style='border-top:none;border-left:none'>46</td>
  <td class=xl67 style='border-top:none;border-left:none'>Joint Low-light
  Enhancement and Super Resolution with Image Underexposure Level Guidance</td>
  <td class=xl67 style='border-top:none;border-left:none'>Mingjie Xu (Beihang
  University); Chaoqun Zhuang (Beihang University); Feifan Lv (Beihang
  University); Feng Lu (Beihang University)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>97</td>
  <td class=xl66 style='border-top:none;border-left:none'>674</td>
  <td class=xl67 style='border-top:none;border-left:none'>Estimating Absorption
  Coefficient from a Single Image via Entropy Minimization</td>
  <td class=xl67 style='border-top:none;border-left:none'>Junya Katahira
  (Kyushu Institute of Technology); Ryo Kawahara (Kyushu Institute of
  Technology); Takahiro Okabe (Kyushu Institute of Technology)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>98</td>
  <td class=xl66 style='border-top:none;border-left:none'>149</td>
  <td class=xl67 style='border-top:none;border-left:none'>Five A+ Network: You
  Only Need 9K Parameters for Underwater Image Enhancement</td>
  <td class=xl67 style='border-top:none;border-left:none'>JingXia Jiang (jimei
  university); Tian Ye (The Hong Kong University of Science and Technology
  (Guangzhou))*; Sixiang Chen (The Hong Kong University of Science and
  Technology (Guangzhou)); Erkang Chen (Jimei University); Yun Liu (Southwest
  University); Shi Jun (XinJiang University); Jinbin Bai (Nanjing University);
  Wenhao Chai (University of Washington)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>99</td>
  <td class=xl66 style='border-top:none;border-left:none'>775</td>
  <td class=xl67 style='border-top:none;border-left:none'>Towards Clip-Free
  Quantized Super-Resolution Networks: How to Tame Representative Images</td>
  <td class=xl67 style='border-top:none;border-left:none'>Alperen Kalay
  (Aselsan Research)*; Bahri Batuhan Bilecen (Aselsan Research); Mustafa
  Ayazoglu (Aselsan Research)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>100</td>
  <td class=xl66 style='border-top:none;border-left:none'>358</td>
  <td class=xl67 style='border-top:none;border-left:none'>RawSeg: Grid Spatial
  and Spectral Attended Semantic Segmentation Based on Raw Bayer Images</td>
  <td class=xl67 style='border-top:none;border-left:none'>Guoyu Lu (University
  of Georgia)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>101</td>
  <td class=xl66 style='border-top:none;border-left:none'>635</td>
  <td class=xl67 style='border-top:none;border-left:none'>Log RGB Images
  Provide Invariance to Intensity and Color Balance Variation for Convolutional
  Networks</td>
  <td class=xl67 style='border-top:none;border-left:none'>Bruce A Maxwell
  (Northeastern University)*; Sumegha Singhania (Northeastern University);
  Heather Fryling (Northeastern University); Haonan Sun (Northeastern
  University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>102</td>
  <td class=xl66 style='border-top:none;border-left:none'>283</td>
  <td class=xl67 style='border-top:none;border-left:none'>MG-MLP: Multi-gated
  MLP for Restoring Images from Spatially Variant Degradations</td>
  <td class=xl67 style='border-top:none;border-left:none'>Jaihyun Koh (Samsung
  Display)*; Jaihyun Lew (Seoul National University); Jangho Lee (Incheon
  National University); Sungroh Yoon (Seoul National University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Low-level and
  Physics-based Vision</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>103</td>
  <td class=xl66 style='border-top:none;border-left:none'>538</td>
  <td class=xl67 style='border-top:none;border-left:none'>Learning Disentangled
  Representations for Environment Inference in Out-of-distribution
  Generalization</td>
  <td class=xl67 style='border-top:none;border-left:none'>Dongqi Li (Beijing
  Jiaotong University); Zhu Teng (Beijing Jiaotong University); Li Qirui
  (AFCtech); Wang Ziyin (AFCtech); Baopeng Zhang (BJTU)*; Jianping Fan (Lenovo)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Machine learning
  (other than deep learning)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>104</td>
  <td class=xl66 style='border-top:none;border-left:none'>750</td>
  <td class=xl67 style='border-top:none;border-left:none'>A2V: A
  Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via
  Two-Phase Training Angiography-to-Venography Translation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Francesco Galati
  (EURECOM)*; Daniele Falcetta (EURECOM); Rosa Cortese (University of Siena);
  Barbara Casolla (CHU Nice); Ferran Prados (University College London); Ninon
  Burgos (CNRS - Paris Brain Institute); Maria A. Zuluaga (EURECOM)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>105</td>
  <td class=xl66 style='border-top:none;border-left:none'>409</td>
  <td class=xl67 style='border-top:none;border-left:none'>AGMDT: Virtual
  Staining of Renal Histology Images with Adjacency-Guided Multi-Domain
  Transfer</td>
  <td class=xl67 style='border-top:none;border-left:none'>Tao Ma (Peking
  University)*; Chao Zhang (Peking University); MIN LU (Peking University
  Health Science Center); Lin Luo (Peking University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>106</td>
  <td class=xl66 style='border-top:none;border-left:none'>84</td>
  <td class=xl67 style='border-top:none;border-left:none'>Spatial and Planar
  Consistency for Semi-Supervised Volumetric Medical Image Segmentation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Yanfeng Zhou
  (Institute of Automation, Chinese Academy of Sciences); yiming huang
  (nstitute of Automation，Chinese Academy of Sciences); Ge Yang (National
  Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy
  of Sciences)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>107</td>
  <td class=xl66 style='border-top:none;border-left:none'>699</td>
  <td class=xl67 style='border-top:none;border-left:none'>Variational
  Autoencoders for Feature Exploration and Malignancy Prediction of Lung
  Lesions</td>
  <td class=xl67 style='border-top:none;border-left:none'>Ben Keel (University
  of Leeds)*; Samuel D. Relton (University of Leeds); Aaron Quyn (University of
  Leeds); David Jayne (University of Leeds)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>108</td>
  <td class=xl66 style='border-top:none;border-left:none'>99</td>
  <td class=xl67 style='border-top:none;border-left:none'>ReSynthDetect: A
  Fundus Anomaly Detection Network with Reconstruction and Synthetic Features</td>
  <td class=xl67 style='border-top:none;border-left:none'>Jingqi Niu (Shanghai
  Jiaotong University)*; Qinji Yu (Shanghai Jiao Tong University); Shiwen Dong
  (Shanghai Jiao Tong University); Zilong Wang (Voxelcloud); Kang Dang
  (Voxelcloud Inc); xiaowei ding (Shanghai Jiao Tong University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>109</td>
  <td class=xl66 style='border-top:none;border-left:none'>411</td>
  <td class=xl67 style='border-top:none;border-left:none'>LACFormer: Toward
  accurate and efficient polyp segmentation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Quan Van Nguyen
  (R&amp;D Lab, Sun Asterisk Inc Vietnam)*; Mai Nguyen ( R&amp;D Lab, Sun
  Asterisk Inc Vietnam); Thanh Tung Nguyen (Sun Asterisk Vietnam); Huy Trịnh
  Quang (Sun-asterisk); Toan Pham Van (R&amp;D Lab, Sun Asterisk Inc Vietnam);
  Linh Bao Doan (Sun* Inc.)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>110</td>
  <td class=xl66 style='border-top:none;border-left:none'>789</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multi-Stain
  Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology
  Whole Slide Images</td>
  <td class=xl67 style='border-top:none;border-left:none'>Amaya Gallagher-Syed
  (Queen Mary University of London)*; Luca Rossi (The Hong Kong Polytechnic
  University); Felice Rivellese (Queen Mary University of London); Costantino
  Pitzalis (Queen Mary University of London); Myles Lewis (Queen Mary
  University of London); Michael Barnes (Queen Mary University of London);
  Gregory Slabaugh (Queen Mary University of London)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>111</td>
  <td class=xl66 style='border-top:none;border-left:none'>462</td>
  <td class=xl67 style='border-top:none;border-left:none'>Enhance Regional Wall
  Segmentation by Style Transfer for Regional Wall Motion Assessment</td>
  <td class=xl67 style='border-top:none;border-left:none'>Kaikai Liu (Northwest
  A&amp;F University); Yiyu Shi (University of Notre Dame); Jian Zhuang
  (Guangdong Provincial People's Hospital); Meiping Huang (Guangdong Provincial
  People's Hospital); Hongwen Fei (Guangdong Provincial People's Hospital);
  Boyang Li (Meta); Jin Hong (Guangdong Provincial People's Hospital); Qing Lu
  (University of Notre Dame); Erlei Zhang (Northwest A&amp;F University);
  Xiaowei Xu (Guangdong Provincial People's Hospital)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Medical and
  biological vision; cell microscopy</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>112</td>
  <td class=xl66 style='border-top:none;border-left:none'>467</td>
  <td class=xl67 style='border-top:none;border-left:none'>Cross-Modal Attention
  for Accurate Pedestrian Trajectory Prediction</td>
  <td class=xl67 style='border-top:none;border-left:none'>Mayssa ZAIER (IMT
  NORD EUROPE)*; Hazem Wannous (University of Lille); Hassen Drira (University
  of Strasbourg); Jacques boonaert (imt lille douai)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Motion estimation and
  tracking</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>113</td>
  <td class=xl66 style='border-top:none;border-left:none'>603</td>
  <td class=xl67 style='border-top:none;border-left:none'>READMem: Robust
  Embedding Association for a Diverse Memory in Unconstrained Video Object
  Segmentation</td>
  <td class=xl67 style='border-top:none;border-left:none'>Stephane Vujasinovic
  (Fraunhofer IOSB)*; Sebastian W Bullinger (Fraunhofer IOSB); Stefan Becker
  (Fraunhofer IOSB); Norbert Scherer-Negenborn (Fraunhofer IOSB); Michael Arens
  (Fraunhofer IOSB); Rainer Stiefelhagen (Karlsruhe Institute of Technology)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Motion estimation and
  tracking</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>114</td>
  <td class=xl66 style='border-top:none;border-left:none'>441</td>
  <td class=xl67 style='border-top:none;border-left:none'>EgoFlowNet: Non-Rigid
  Scene Flow from Point Clouds with Ego-Motion Support</td>
  <td class=xl67 style='border-top:none;border-left:none'>Ramy Battrawy
  (DFKI)*; René Schuster (DFKI); Didier Stricker (DFKI)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Motion estimation and
  tracking</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>115</td>
  <td class=xl66 style='border-top:none;border-left:none'>813</td>
  <td class=xl67 style='border-top:none;border-left:none'>Learning Tri-modal
  Embeddings for Zero-Shot Soundscape Mapping</td>
  <td class=xl67 style='border-top:none;border-left:none'>Subash Khanal
  (Washington University in Saint Louis)*; Srikumar Sastry (Washington
  University in St. Louis); Aayush Dhakal (Washington University in St Louis);
  Nathan Jacobs (Washington University in St. Louis)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multimodal learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>116</td>
  <td class=xl66 style='border-top:none;border-left:none'>624</td>
  <td class=xl67 style='border-top:none;border-left:none'>Text-to-Motion
  Synthesis using Discrete Diffusion Model</td>
  <td class=xl67 style='border-top:none;border-left:none'>Ankur Chemburkar (USC
  Institute for Creative Technologies)*; Shuhong Lu (USC Institute for Creative
  Technologies); Andrew Feng (USC Institute for Creative Technologies)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multimodal learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>117</td>
  <td class=xl66 style='border-top:none;border-left:none'>460</td>
  <td class=xl67 style='border-top:none;border-left:none'>AMA: Adaptive Memory
  Augmentation for Enhancing Image Captioning</td>
  <td class=xl67 style='border-top:none;border-left:none'>Shuang Cheng
  (Institute of Computing Technology, Chinese Academy of Sciences)*; Jian Ye
  (Institute of Computing Technology, CAS)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multimodal learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>118</td>
  <td class=xl66 style='border-top:none;border-left:none'>326</td>
  <td class=xl67 style='border-top:none;border-left:none'>X-PDNet: Accurate
  Joint Plane Instance Segmentation and Monocular Depth Estimation with
  Cross-Task Attention and Boundary Correction</td>
  <td class=xl67 style='border-top:none;border-left:none'>Duc Cao Dinh
  (Computer Vision Lab, Hanyang University)*; Jongwoo Lim (Hanyang University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multimodal learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>119</td>
  <td class=xl66 style='border-top:none;border-left:none'>381</td>
  <td class=xl67 style='border-top:none;border-left:none'>Zero-shot Composed
  Text-Image Retrieval</td>
  <td class=xl67 style='border-top:none;border-left:none'>Yikun Liu (Beijing
  University of Posts and Telecommunications)*; Jiangchao Yao (Cooperative
  Medianet Innovation Center, Shang hai Jiao Tong University); Yan-Feng Wang
  (Cooperative medianet innovation center of Shanghai Jiao Tong University); Ya
  Zhang (Cooperative Medianet Innovation Center, Shang hai Jiao Tong
  University); Weidi Xie (Shanghai Jiao Tong University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multimodal learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>120</td>
  <td class=xl66 style='border-top:none;border-left:none'>666</td>
  <td class=xl67 style='border-top:none;border-left:none'>E2SAM: A Pipeline for
  Efficiently Extending SAM's Capability on Cross-Modality Data via Knowledge
  Inheritance</td>
  <td class=xl67 style='border-top:none;border-left:none'>Su sundingkai
  (Beijing University of Posts and Telecommunications); Mengqiu Xu (Beijing
  University of Posts and Telecommunications); Kaixin Chen (Beijing University
  of Posts and Telecommunications); Ming Wu (Beijing University of Posts and
  Telecommunications)*; Chuang Zhang (Beijing University of Posts and
  Telecommunications)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Multimodal learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>121</td>
  <td class=xl66 style='border-top:none;border-left:none'>478</td>
  <td class=xl67 style='border-top:none;border-left:none'>Conditional
  Generation from Pre-Trained Diffusion Models using Denoiser Representations</td>
  <td class=xl67 style='border-top:none;border-left:none'>Alexandros Graikos
  (Stony Brook University)*; Srikar Yellapragada (Stony Brook University);
  Dimitris Samaras (Stony Brook University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Neural generative
  models</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>122</td>
  <td class=xl66 style='border-top:none;border-left:none'>715</td>
  <td class=xl67 style='border-top:none;border-left:none'>Bridging the Gap:
  Enhancing the Utility of Synthetic Data via Post-Processing Techniques</td>
  <td class=xl67 style='border-top:none;border-left:none'>Eugenio Lomurno
  (Politecnico di Milano)*; Andrea Lampis (Politecnico di Milano); Matteo
  Matteucci (Politecnico di Milano)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Neural generative
  models</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>123</td>
  <td class=xl66 style='border-top:none;border-left:none'>787</td>
  <td class=xl67 style='border-top:none;border-left:none'>TD-GEM: Text-Driven
  Garment Editing Mapper</td>
  <td class=xl67 style='border-top:none;border-left:none'>Reza Dadfar (KTH);
  Sanaz Sabzevari (KTH University)*; Marten Bjorkman (KTH); Danica Kragic (KTH
  Royal Institute of Technology)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Neural generative
  models</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>124</td>
  <td class=xl66 style='border-top:none;border-left:none'>243</td>
  <td class=xl67 style='border-top:none;border-left:none'>Class-Continuous
  Conditional Generative Neural Radiance Field</td>
  <td class=xl67 style='border-top:none;border-left:none'>Jiwook Kim (Chung-Ang
  University)*; Minhyeok Lee (Chung-Ang University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Neural generative
  models</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>125</td>
  <td class=xl66 style='border-top:none;border-left:none'>22</td>
  <td class=xl67 style='border-top:none;border-left:none'>Locality-Aware
  Hyperspectral Classification</td>
  <td class=xl67 style='border-top:none;border-left:none'>Fangqin Zhou
  (Technology University of Eindhoven); Mert Kilickaya (Eindhoven University of
  Technology)*; Joaquin Vanschoren (Eindhoven University of Technology)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Photogrammetry and
  remote sensing</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>126</td>
  <td class=xl66 style='border-top:none;border-left:none'>4</td>
  <td class=xl67 style='border-top:none;border-left:none'>Instance Mask Growing
  on Leaf</td>
  <td class=xl67 style='border-top:none;border-left:none'>Chuang Yang
  (Northwestern Polytechnical University); Haozhao Ma (Northwestern
  Polytechnical University); Qi Wang (Northwestern Polytechnical University)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Recognition:
  Categorization and Instance recognition</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>127</td>
  <td class=xl66 style='border-top:none;border-left:none'>135</td>
  <td class=xl67 style='border-top:none;border-left:none'>Infinite Class Mixup</td>
  <td class=xl67 style='border-top:none;border-left:none'>Thomas Mensink
  (Google Research); Pascal Mettes (University of Amsterdam)*</td>
  <td class=xl67 style='border-top:none;border-left:none'>Recognition:
  Categorization and Instance recognition</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>128</td>
  <td class=xl66 style='border-top:none;border-left:none'>375</td>
  <td class=xl67 style='border-top:none;border-left:none'>Building A Mobile
  Text Recognizer via Truncated SVD-based Knowledge Distillation-Guided NAS</td>
  <td class=xl67 style='border-top:none;border-left:none'>Weifeng Lin (South
  China University of Technology); Canyu Xie (South China University of
  Technology); Dezhi Peng (South China University of Technology); Jiapeng Wang
  (South China University of Technology); Lianwen Jin (South China University
  of Technology)*; Wei Ding (Alibaba Group); Cong Yao (Alibaba DAMO Academy);
  Mengchao He (DAMO Academy, Alibaba Group)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Recognition:
  Categorization and Instance recognition</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>129</td>
  <td class=xl66 style='border-top:none;border-left:none'>119</td>
  <td class=xl67 style='border-top:none;border-left:none'>Integrating Transient
  and Long-term Physical States for Depression Intelligent Diagnosis</td>
  <td class=xl67 style='border-top:none;border-left:none'>Ke Wu (Beihang
  University); Han Jiang (Beihang University)*; Li Kuang (Beihang University);
  Yixuan Wang (Beihang University); Huaiqian Ye (Beihang University); Yuanbo He
  (State Key Laboratory of Virtual Reality Technology and Systems, Beihang
  University)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Recognition:
  Categorization and Instance recognition</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>130</td>
  <td class=xl66 style='border-top:none;border-left:none'>320</td>
  <td class=xl67 style='border-top:none;border-left:none'>Learning Unified
  Representations for Multi-Resolution Face Recognition</td>
  <td class=xl67 style='border-top:none;border-left:none'>Hulingxiao He (School
  of Automation,Beijing Institute of Technology); Wu Yuan (School of Computer
  Science,Beijing Institute of Technology)*; Yidian Huang (Beijing Institute of
  Technology); Shilong Zhao (Beijing Institute of Technology); Wen Yuan (State
  Key Laboratory of Resources and Environmental Information System, Institute
  of Geographic Sciences and Natural Resources Research, CAS); Hanqing Li
  (University of the Chinese Academy of Sciences)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Recognition:
  Categorization and Instance recognition</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl66 style='height:15.75pt;border-top:none;border-left:
  none'>131</td>
  <td class=xl66 style='border-top:none;border-left:none'>102</td>
  <td class=xl67 style='border-top:none;border-left:none'>ReCoT: Regularized
  Co-Training for Facial Action Unit Recognition with Noisy Labels</td>
  <td class=xl67 style='border-top:none;border-left:none'>Yifan Li (Michigan
  State University); Hu Han (Institute of Computing Technology, Chinese Academy
  of Sciences)*; Shiguang Shan (Institute of Computing Technology, Chinese
  Academy of Sciences); zhilong ji (Tomorrow Advancing Life); Jinfeng Bai
  (Tomorrow Advance Life); Xilin Chen (Institute of Computing Technology,
  Chinese Academy of Sciences)</td>
  <td class=xl67 style='border-top:none;border-left:none'>Recognition:
  Categorization and Instance recognition</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=69 height=1380 class=xl75 style='border-bottom:.5pt solid black;
  height:1086.75pt;border-top:none'>Thursday</td>
  <td class=xl68 style='border-top:none;border-left:none'>132</td>
  <td class=xl68 style='border-top:none;border-left:none'>272</td>
  <td class=xl69 style='border-top:none;border-left:none'>SMPLitex: A
  Generative Model and Dataset for 3D Human Texture Estimation from Single
  Image</td>
  <td class=xl69 style='border-top:none;border-left:none'>Dan Casas
  (Universidad Rey Juan Carlos)*; Marc Comino Trinidad (Universidad Rey Juan
  Carlos)</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D from a single
  image and shape-from-x</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>133</td>
  <td class=xl68 style='border-top:none;border-left:none'>800</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mobile Vision
  Transformer-based Visual Object Tracking</td>
  <td class=xl69 style='border-top:none;border-left:none'>Goutam Yelluru Gopal
  (Concordia University)*; Maria Amer (Concordia University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Object pose
  estimation and tracking</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>134</td>
  <td class=xl68 style='border-top:none;border-left:none'>444</td>
  <td class=xl69 style='border-top:none;border-left:none'>Semi-Supervised
  Domain Generalization for Detection via Language-Guided Feature Alignment</td>
  <td class=xl69 style='border-top:none;border-left:none'>Sina Malakouti
  (University of Pittsburgh)*; Adriana Kovashka (University of Pittsburgh)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Detection</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>135</td>
  <td class=xl68 style='border-top:none;border-left:none'>94</td>
  <td class=xl69 style='border-top:none;border-left:none'>Likelihood-based
  Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models</td>
  <td class=xl69 style='border-top:none;border-left:none'>Joseph S Goodier
  (University of Bath)*; Neill Campbell (University of Bath)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Detection</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>136</td>
  <td class=xl68 style='border-top:none;border-left:none'>323</td>
  <td class=xl69 style='border-top:none;border-left:none'>Point-to-RBox Network
  for Oriented Object Detection via Single Point Supervision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yucheng Wang (WuHan
  University)*; Chu He (Wuhan University); Xi Chen (Wuhan university)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Detection</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>137</td>
  <td class=xl68 style='border-top:none;border-left:none'>310</td>
  <td class=xl69 style='border-top:none;border-left:none'>Widely Applicable
  Strong Baseline for Sports Ball Detection and Tracking</td>
  <td class=xl69 style='border-top:none;border-left:none'>Shuhei Tarashima (NTT
  Communications Corporation)*; Norio Tagawa (Tokyo Metropolitan University);
  Muhammad Abdul Haq (Tokyo Metropolitan University); Wang Yushan (Tokyo
  Metropolitan University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Detection</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>138</td>
  <td class=xl68 style='border-top:none;border-left:none'>93</td>
  <td class=xl69 style='border-top:none;border-left:none'>Open-Vocabulary
  Object Detection with Meta Prompt Representation and Instance Contrastive
  Optimization</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zhao Wang (The
  Chinese University of Hong Kong)*; Aoxue Li (Noah's Ark Lab); Fengwei Zhou
  (Huawei Noah's Ark Lab); Zhenguo Li (Huawei Noah's Ark Lab); DOU QI (The
  Chinese University of Hong Kong)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Detection</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>139</td>
  <td class=xl68 style='border-top:none;border-left:none'>707</td>
  <td class=xl69 style='border-top:none;border-left:none'>SWIN-RIND: Edge
  Detection for Reflectance, Illumination, Normal and Depth Discontinuity with
  Swin Transformer</td>
  <td class=xl69 style='border-top:none;border-left:none'>LUN MIAO (The
  University of Tokyo)*; Takeshi Oishi (The University of Tokyo); Ryoichi
  Ishikawa (The university of Tokyo)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Detection</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>140</td>
  <td class=xl68 style='border-top:none;border-left:none'>33</td>
  <td class=xl69 style='border-top:none;border-left:none'>Scale Adaptive
  Network for Partial Person Re-identification: Counteracting Scale Variance</td>
  <td class=xl69 style='border-top:none;border-left:none'>HongYu Chen
  (Northwestern Polytechnical University)*; BingLiang Jiao (Northwestern
  Polytechnical University ); Liying Gao ( Northwestern Polytechnical
  University); Peng Wang (Northwestern Polytechnical University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Retrieval</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>141</td>
  <td class=xl68 style='border-top:none;border-left:none'>608</td>
  <td class=xl69 style='border-top:none;border-left:none'>Object-Centric
  Open-Vocabulary Image-Retrieval with Sparse Features</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hila Levi (General
  Motors)*; Guy Heller (General Motors); Dan Levi (General Motors); Ethan
  Fetaya (Bar Ilan University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition:
  Retrieval</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>142</td>
  <td class=xl68 style='border-top:none;border-left:none'>353</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adapting
  Self-Supervised Representations to Multi-Domain Setups</td>
  <td class=xl69 style='border-top:none;border-left:none'>Neha Kalibhat
  (University of Maryland - College Park)*; Sam Sharpe (Capital One); Jeremy
  Goodsitt (Capital One); C. Bayan Bruss (Capital One); Soheil Feizi
  (University of Maryland)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Representation
  Learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>143</td>
  <td class=xl68 style='border-top:none;border-left:none'>111</td>
  <td class=xl69 style='border-top:none;border-left:none'>SeqCo-DETR: Sequence
  Consistency Training for Self-Supervised Object Detection with Transformers</td>
  <td class=xl69 style='border-top:none;border-left:none'>Guoqiang Jin
  (SenseTime Research)*; Fan Yang (中国科学院自动化研究所); Mingshan Sun (SenseTime
  Research ); Ruyi Zhao (Tongji University); Yakun Liu (SenseTime Research);
  Wei Li (SenseTime Research); Tianpeng Bao (SenseTime Research); Liwei Wu
  (SenseTime Research); Xingyu ZENG (SenseTime Group Limited); Rui Zhao
  (SenseTime Group Limited)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Representation
  Learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>144</td>
  <td class=xl68 style='border-top:none;border-left:none'>433</td>
  <td class=xl69 style='border-top:none;border-left:none'>Variational
  Autoencoders with Decremental Information Bottleneck for Disentanglement</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jiantao Wu
  (University of Surrey)*; Shentong Mo (Carnegie Mellon University); Xingshen
  Zhang (University of Jinan); Muhammad Awais (University of Surrey); Sara
  Ahmed (University of surrey); Zhenhua Feng (University of Surrey); Lin Wang
  (University of Jinan); Xiang Yang (Zhejiang Mingyi Technology Co., Ltd.)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Representation
  Learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>145</td>
  <td class=xl68 style='border-top:none;border-left:none'>351</td>
  <td class=xl69 style='border-top:none;border-left:none'>Cross-domain Semantic
  Decoupling for Weakly-Supervised Semantic Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zaiquan Yang (Beihang
  University)*; Zhanghan Ke (City University of Hong Kong); Gerhard P. Hancke
  (City University of Hong Kong); Rynson W.H. Lau (City University of Hong
  Kong)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Representation
  Learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>146</td>
  <td class=xl68 style='border-top:none;border-left:none'>394</td>
  <td class=xl69 style='border-top:none;border-left:none'>Unifying Synergies
  between Self-supervised Learning and Dynamic Computation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Tarun Krishna (DCU)*;
  Ayush K. Rai (Dublin City University); Eric Arazo (Insight Centre for Data
  Analytics (DCU)); Paul Albert (Insight Centre for Data Analytics (DCU));
  Alexandru F Drimbarean (Xperi); Alan Smeaton (Insight Centre for Data
  Analytics, Dublin City University); Kevin McGuinness (DCU); Noel O Connor
  (Home)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Representation
  Learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>147</td>
  <td class=xl68 style='border-top:none;border-left:none'>226</td>
  <td class=xl69 style='border-top:none;border-left:none'>PanoMixSwap –
  Panorama Mixing via Structural Swapping for Indoor Scene Understanding</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yu-Cheng Hsieh
  (National Tsing Hua University)*; Cheng Sun (National Tsing Hua University);
  Suraj Dengale (National Tsing Hua University); Min Sun (NTHU)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Scene Analysis and
  Understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>148</td>
  <td class=xl68 style='border-top:none;border-left:none'>499</td>
  <td class=xl69 style='border-top:none;border-left:none'>Clustered Saliency
  Prediction</td>
  <td class=xl69 style='border-top:none;border-left:none'>Rezvan Sherkati
  (McGill University)*; James J. Clark (McGill University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Scene Analysis and
  Understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>149</td>
  <td class=xl68 style='border-top:none;border-left:none'>77</td>
  <td class=xl69 style='border-top:none;border-left:none'>One-stage Progressive
  Dichotomous Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jing Zhu (Samsung
  Research America)*; Karim Ahmed (Samsung Research America); Wenbo Li (Samsung
  Research America); Yilin Shen (Samsung Research America); Hongxia Jin
  (Samsung Research America)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>150</td>
  <td class=xl68 style='border-top:none;border-left:none'>81</td>
  <td class=xl69 style='border-top:none;border-left:none'>Towards Robust
  Few-shot Point Cloud Semantic Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yating Xu (National
  University of Singapore)*; Na Zhao (SUTD); Gim Hee Lee (National University
  of Singapore)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>151</td>
  <td class=xl68 style='border-top:none;border-left:none'>815</td>
  <td class=xl69 style='border-top:none;border-left:none'>Text and Click inputs
  for unambiguous open vocabulary instance segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vighnesh N Birodkar
  (Google)*; Jonathan Huang (Google); Meera Hahn (Google); Irfan Essa (Georgia
  Institute of Technology); Nikolai Warner (Georgia Tech)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>152</td>
  <td class=xl68 style='border-top:none;border-left:none'>868</td>
  <td class=xl69 style='border-top:none;border-left:none'>Multi-Scale Cross
  Contrastive Learning for Semi-Supervised Medical Image Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Qianying Liu
  (University of Glasgow)*; Xiao Gu (Imperial College London); Paul Henderson
  (University of Glasgow); Fani Deligianni (University of Glasgow)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>153</td>
  <td class=xl68 style='border-top:none;border-left:none'>623</td>
  <td class=xl69 style='border-top:none;border-left:none'>Superpixel Positional
  Encoding to Improve ViT-based Semantic Segmentation Models</td>
  <td class=xl69 style='border-top:none;border-left:none'>Roberto Amoroso
  (University of Modena and Reggio Emilia)*; Matteo Tomei (Prometeia); Lorenzo
  Baraldi (University of Modena and Reggio Emilia); Rita Cucchiara (Università
  di Modena e Reggio Emilia)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>154</td>
  <td class=xl68 style='border-top:none;border-left:none'>767</td>
  <td class=xl69 style='border-top:none;border-left:none'>Label-guided
  Real-time Fusion Network forRGB-T Semantic Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zengrong Lin (Sun
  Yat-sen University); Baihong Lin (University of Electronic Science and
  Technology of China)*; Yulan Guo (Sun Yat-sen University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>155</td>
  <td class=xl68 style='border-top:none;border-left:none'>523</td>
  <td class=xl69 style='border-top:none;border-left:none'>SHLS: Superfeatures
  learned from still images for self-supervised VOS</td>
  <td class=xl69 style='border-top:none;border-left:none'>Marcelo M Santos
  (UFBA)*; Jefferson Fontinele da Silva (University Federal of Maranhão);
  Luciano Oliveira (UFBA)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>156</td>
  <td class=xl68 style='border-top:none;border-left:none'>530</td>
  <td class=xl69 style='border-top:none;border-left:none'>AutoSAM: Adapting SAM
  to Medical Images by Overloading the Prompt Encoder</td>
  <td class=xl69 style='border-top:none;border-left:none'>Tal Shaharbany (Tel
  Aviv University)*; ‪Aviad Dahan‬‏ (Tel Aviv University); Raja Giryes (Tel
  Aviv University); Lior Wolf (Tel Aviv University, Israel)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>157</td>
  <td class=xl68 style='border-top:none;border-left:none'>719</td>
  <td class=xl69 style='border-top:none;border-left:none'>EyeGuide - From Gaze
  Data to Instance Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jacqueline Kockwelp
  (University of Münster); Joerg Gromoll (CeRA); Joachim Wistuba (Centre of
  Reproductive Medicine and Andrology); Benjamin Risse (University of Münster)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Segmentation,
  grouping and shape analysis</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>158</td>
  <td class=xl68 style='border-top:none;border-left:none'>908</td>
  <td class=xl69 style='border-top:none;border-left:none'>Class-Imbalanced
  Semi-Supervised Learning with Inverse Auxiliary Classifier</td>
  <td class=xl69 style='border-top:none;border-left:none'>Tiansong Jiang
  (Nanjing University of Science and Technology)*; Sheng Wan (Nanjing
  university of science and technology); Chen Gong (Nanjing University of
  Science and Technology)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>159</td>
  <td class=xl68 style='border-top:none;border-left:none'>899</td>
  <td class=xl69 style='border-top:none;border-left:none'>C3: Cross-instance
  guided Contrastive Clustering</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mohammadreza Sadeghi
  (McGill University); Hadi Hojjati (McGill University); Narges Armanfard
  (McGill University; Mila - Quebec AI Institute)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>160</td>
  <td class=xl68 style='border-top:none;border-left:none'>676</td>
  <td class=xl69 style='border-top:none;border-left:none'>BFC-BL: Few-Shot
  Classification and Segmentation combining Bi-directional Feature Correlation
  and Boundary constraint</td>
  <td class=xl69 style='border-top:none;border-left:none'>Haibiao Yang
  (Guangdong University of Technology)*; Zeng Bi (Guangdong University of
  Technology); Pengfei Wei (Guangdong University of Technology); Jianqi Liu
  (Guangdong University of Technology)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>161</td>
  <td class=xl68 style='border-top:none;border-left:none'>259</td>
  <td class=xl69 style='border-top:none;border-left:none'>Prototype-Aware
  Contrastive Knowledge Distillation for Few-Shot Anomaly Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zhihao Gu (Shanghai
  Jiao Tong University)*; Taihai Yang (East China Normal University); Lizhuang
  Ma (Shanghai Jiao Tong University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>162</td>
  <td class=xl68 style='border-top:none;border-left:none'>837</td>
  <td class=xl69 style='border-top:none;border-left:none'>Domain-Adaptive
  Semantic Segmentation with Memory-Efficient Cross-Domain Transformers</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ruben Mascaro (ETH
  Zurich)*; Lucas Teixeira (ETH Zurich); Margarita Chli (ETH Zurich)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>163</td>
  <td class=xl68 style='border-top:none;border-left:none'>117</td>
  <td class=xl69 style='border-top:none;border-left:none'>Detect, Augment,
  Compose, and Adapt: Four Steps for Unsupervised Domain Adaptation in Object
  Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mohamed Lamine
  Mekhalfi (Fondazione Bruno Kessler)*; Davide Boscaini (Fondazione Bruno
  Kessler); Fabio Poiesi (Fondazione Bruno Kessler)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>164</td>
  <td class=xl68 style='border-top:none;border-left:none'>215</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hierarchical
  Quantization Consistency for Fully Unsupervised Image Retrieval</td>
  <td class=xl69 style='border-top:none;border-left:none'>Guile Wu (Noah’s Ark
  Lab); Chao Zhang (Toshiba Europe Limited)*; Stephan Liwicki (Toshiba Europe
  Limited)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>165</td>
  <td class=xl68 style='border-top:none;border-left:none'>297</td>
  <td class=xl69 style='border-top:none;border-left:none'>Exploring the Limits
  of Deep Image Clustering using Pretrained Models</td>
  <td class=xl69 style='border-top:none;border-left:none'>Nikolas Adaloglou
  (HHU)*; Felix Michels (HHU); Hamza Kalisch (HHU); Markus Kollmann (HHU)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>166</td>
  <td class=xl68 style='border-top:none;border-left:none'>471</td>
  <td class=xl69 style='border-top:none;border-left:none'>Enhancing
  Interpretable Object Abstraction via Clustering-based Slot Initialization</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ning Gao (Bosch
  Center for Artificial Intelligence (BCAI))*; Bernard Hohmann (Karlsruhe
  Institute of Technology); Gerhard Neumann (Karlsruhe Institute of Technology
  (KIT), Karlsruhe, Germany)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>167</td>
  <td class=xl68 style='border-top:none;border-left:none'>240</td>
  <td class=xl69 style='border-top:none;border-left:none'>StereoFlowGAN:
  Co-training for Stereo and Flow with Unsupervised Domain Adaptation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zhexiao Xiong
  (Washington University in St. Louis)*; Feng Qiao (RWTH Aachen University); Yu
  Zhang (Bastian Solutions); Nathan Jacobs (Washington University in St. Louis)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Self-, semi-, meta-,
  unsupervised learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>168</td>
  <td class=xl68 style='border-top:none;border-left:none'>633</td>
  <td class=xl69 style='border-top:none;border-left:none'>Multi-Target Domain
  Adaptation with Class-Wise Attribute Transfer in Semantic Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Changjae Kim (DGIST);
  Seunghun Lee (DGIST)*; Sunghoon Im (DGIST)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>169</td>
  <td class=xl68 style='border-top:none;border-left:none'>858</td>
  <td class=xl69 style='border-top:none;border-left:none'>Weakly-supervised
  Spatially Grounded Concept Learner for Few-Shot Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Gaurav Bhatt (The
  University of British Columbia)*; Deepayan Das (IIT-H); Leonid Sigal
  (University of British Columbia); Vineeth N Balasubramanian (Indian Institute
  of Technology, Hyderabad)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>170</td>
  <td class=xl68 style='border-top:none;border-left:none'>12</td>
  <td class=xl69 style='border-top:none;border-left:none'>RestNet: Boosting
  Cross-Domain Few-Shot Segmentation with Residual Transformation Network</td>
  <td class=xl69 style='border-top:none;border-left:none'>Xinyang Huang
  (Beijing University of Posts and Telecommunications)*; Chuang Zhu (Beijing
  University of Posts and Telecommunications ); Wenkai Chen (Beijing University
  of Posts and Telecommunications)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>171</td>
  <td class=xl68 style='border-top:none;border-left:none'>18</td>
  <td class=xl69 style='border-top:none;border-left:none'>Random Word Data
  Augmentation with CLIP for Zero-Shot Anomaly Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Masato Tamura
  (Hitachi America, Ltd.)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>172</td>
  <td class=xl68 style='border-top:none;border-left:none'>202</td>
  <td class=xl69 style='border-top:none;border-left:none'>Few-Shot Anomaly
  Detection with Adversarial Loss for Robust Feature Representations</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jae Young Lee
  (KAIST)*; Wonjun Lee (University of Science and Technology ); Jaehyun Choi
  (KAIST); Yongkwi LEE (ETRI); Young Seog Yoon (Electronics and
  Telecommunications Research Institute)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>173</td>
  <td class=xl68 style='border-top:none;border-left:none'>330</td>
  <td class=xl69 style='border-top:none;border-left:none'>Fine-grained Few-shot
  Recognition by Deep Object Parsing</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ruizhao Zhu (Boston
  University)*; Pengkai Zhu (Amazon Web Services); Samarth Mishra (Boston
  University); Venkatesh Saligrama (Boston University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>174</td>
  <td class=xl68 style='border-top:none;border-left:none'>762</td>
  <td class=xl69 style='border-top:none;border-left:none'>Novel Regularization
  via Logit Weight Repulsion for Long-Tailed Classification</td>
  <td class=xl69 style='border-top:none;border-left:none'>Taegil Ha (Seoul
  National University)*; Seulki Park (Seoul National University); Jin Young
  Choi (Seoul National University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>175</td>
  <td class=xl68 style='border-top:none;border-left:none'>292</td>
  <td class=xl69 style='border-top:none;border-left:none'>Generating
  Pseudo-labels Adaptively for Few-shot Model-Agnostic Meta-Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Guodong Liu (Huazhong
  University of Science and Technology); Tongling Wang (Huazhong University of
  Science and Technology); Shuoxi Zhang (Huazhong University of Science and
  Technology); Kun He (Huazhong University of Science and Technology)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>176</td>
  <td class=xl68 style='border-top:none;border-left:none'>452</td>
  <td class=xl69 style='border-top:none;border-left:none'>Domain-Aware
  Augmentations for Unsupervised Online General Continual Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Nicolas Michel
  (LIGM)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>177</td>
  <td class=xl68 style='border-top:none;border-left:none'>534</td>
  <td class=xl69 style='border-top:none;border-left:none'>Dual Feature
  Augmentation Network for Generalization Zero-shot Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Lei Xiang (Nanjing
  University of Information Science and Technology )*; Yuan Zhou (Nanjing
  University of Information Science and Technology); Haoran Duan (Durham
  University); Yang Long (Durham University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>178</td>
  <td class=xl68 style='border-top:none;border-left:none'>264</td>
  <td class=xl69 style='border-top:none;border-left:none'>Predictive
  Consistency Learning for Long-Tailed Recognition</td>
  <td class=xl69 style='border-top:none;border-left:none'>Nan Kang (Key
  Laboratory of Intelligent Information Processing of Chinese Academy of
  Sciences (CAS))*; Hong Chang (Chinese Academy of Sciences); Bingpeng MA
  (University of Chinese Academy of Sciences); Shutao Bai (Institute of
  Computing Technology, Chinese Academy of Sciences); Shiguang Shan (Institute
  of Computing Technology, Chinese Academy of Sciences); Xilin Chen (Institute
  of Computing Technology, Chinese Academy of Sciences)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Transfer, low-shot,
  continual, long-tail learning</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>179</td>
  <td class=xl68 style='border-top:none;border-left:none'>542</td>
  <td class=xl69 style='border-top:none;border-left:none'>Temporal-aware
  Hierarchical Mask Classification for Video Semantic Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zhaochong An (ETH
  Zurich); Guolei Sun (ETH Zurich)*; Zongwei WU (Univ. Bourgogne Franche-Comte,
  France); Hao Tang (ETH Zurich); Luc Van Gool (ETH Zurich)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Video analysis and
  Understanding</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>180</td>
  <td class=xl68 style='border-top:none;border-left:none'>25</td>
  <td class=xl69 style='border-top:none;border-left:none'>Motion and
  Context-Aware Audio-Visual Conditioned Video Prediction</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yating Xu (National
  University of Singapore)*; Conghui Hu (National University of Singapore); Gim
  Hee Lee (National University of Singapore)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and audio</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>181</td>
  <td class=xl68 style='border-top:none;border-left:none'>144</td>
  <td class=xl69 style='border-top:none;border-left:none'>Dual Attention for
  Audio-Visual Speech Enhancement with Facial Cues</td>
  <td class=xl69 style='border-top:none;border-left:none'>Fexiang Wang (ICT,
  UCAS)*; Shuang Yang (ICT, CAS); Shiguang Shan (Institute of Computing
  Technology, Chinese Academy of Sciences); Xilin Chen (Institute of Computing
  Technology, Chinese Academy of Sciences)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and audio</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>182</td>
  <td class=xl68 style='border-top:none;border-left:none'>367</td>
  <td class=xl69 style='border-top:none;border-left:none'>How Can Contrastive
  Pre-training Benefit Audio-Visual Segmentation? A Study from Supervised and
  Zero-shot Perspectives</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jiarui Yu (USTC)*;
  Haoran Li (University of Science and Technology of China); Yanbin Hao
  (University of Science and Technology of China); Wu Jinmeng (Wuhan Institute
  of Technology); Tong Xu (University of Science and Technology of China); Shuo
  Wang (University of Science and Technology of China); Xiangnan He (University
  of Science and Technology of China)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and audio</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>183</td>
  <td class=xl68 style='border-top:none;border-left:none'>139</td>
  <td class=xl69 style='border-top:none;border-left:none'>Continuous Levels of
  Detail for Light Field Networks</td>
  <td class=xl69 style='border-top:none;border-left:none'>David Li (University
  of Maryland College Park)*; Brandon Yushan Feng (University of Maryland,
  College Park); Amitabh Varshney (University of Maryland)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and graphics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>184</td>
  <td class=xl68 style='border-top:none;border-left:none'>347</td>
  <td class=xl69 style='border-top:none;border-left:none'>SRNet: Striped
  Pyramid Pooling and Relational Transformer for Retinal Vessel Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Wei Yan (College of
  Computer Science and Engineering, Northwest Normal University)*; Yun Jiang
  (College of Computer Science and Engineering, Northwest Normal University);
  Zequn Zhang (Northwest Normal University ); Yao Yan (College of Computer
  Science and Engineering, Northwest Normal University); Bingxi Liu (Northwest
  Normal University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and graphics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>185</td>
  <td class=xl68 style='border-top:none;border-left:none'>451</td>
  <td class=xl69 style='border-top:none;border-left:none'>Complex Scene Image
  Editing by Scene Graph Comprehension</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zhongping Zhang
  (Boston University)*; Huiwen He (Boston University); Bryan Plummer (Boston
  University); Zhenyu Liao (Kwai Inc); Huayan Wang (Kuaishou Technology)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>186</td>
  <td class=xl68 style='border-top:none;border-left:none'>314</td>
  <td class=xl69 style='border-top:none;border-left:none'>GOPro: Generate and
  Optimize Prompts in CLIP using Self-Supervised Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mainak Singha (Indian
  Institute of Technology Bombay)*; Ankit Jha (Indian Institute of Technology
  Bombay); Biplab Banerjee (Indian Institute of Technology, Bombay)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>187</td>
  <td class=xl68 style='border-top:none;border-left:none'>182</td>
  <td class=xl69 style='border-top:none;border-left:none'>BDC-Adapter: Brownian
  Distance Covariance for Better Vision-Language Reasoning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yi Zhang (Southern
  University of Science and Technology); Ce Zhang (Carnegie Mellon University);
  Zihan Liao (Southern University of Science and Technology); Yushun Tang
  (Southern University of Science and Technology); Zhihai He (Southern
  University of Science and Technology)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>188</td>
  <td class=xl68 style='border-top:none;border-left:none'>510</td>
  <td class=xl69 style='border-top:none;border-left:none'>Open-world
  Text-specifed Object Counting</td>
  <td class=xl69 style='border-top:none;border-left:none'>Niki Amini-Naieni
  (University of Oxford)*; Kiana Amini-Naieni (University of California,
  Davis); Tengda Han (University of Oxford); Andrew Zisserman (University of
  Oxford)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>189</td>
  <td class=xl68 style='border-top:none;border-left:none'>650</td>
  <td class=xl69 style='border-top:none;border-left:none'>Towards Debiasing
  Frame Length Bias in Text-Video Retrieval via Causal Intervention</td>
  <td class=xl69 style='border-top:none;border-left:none'>Burak Satar (Nanyang
  Technological University)*; Hongyuan Zhu (Institute for Infocomm, Research
  Agency for Science, Technology and Research (A*STAR) Singapore); Hanwang
  Zhang (Nanyang Technological University); Joo-Hwee Lim (Institute for
  Infocomm Research)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>190</td>
  <td class=xl68 style='border-top:none;border-left:none'>229</td>
  <td class=xl69 style='border-top:none;border-left:none'>Weakly-Supervised
  Visual-Textual Grounding with Semantic Prior Refinement</td>
  <td class=xl69 style='border-top:none;border-left:none'>Davide Rigoni
  (University of Padua); Luca Parolari (University of Padova); Luciano Serafini
  (Fondazione Bruno Kessler); Alessandro Sperduti (Università di Padova (IT));
  Lamberto Ballan (University of Padova)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>191</td>
  <td class=xl68 style='border-top:none;border-left:none'>596</td>
  <td class=xl69 style='border-top:none;border-left:none'>Generating
  Context-Aware Natural Answers for Questions in 3D Scenes</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mohammed Munzer
  Dwedari (Technical University of Munich)*; Matthias Niessner (Technical
  University of Munich); Zhenyu Chen (Technical University of Munich)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and language</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>192</td>
  <td class=xl68 style='border-top:none;border-left:none'>378</td>
  <td class=xl69 style='border-top:none;border-left:none'>Neural Feature
  Filtering for Faster Structure-from-Motion Localisation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Alexandros Rotsidis
  (University of Bath)*; Yuxin Wang (École polytechnique fédérale de Lausanne);
  Yiorgos Chrysanthou (CYENS Centre of Excellence); Christian Richardt (Meta)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision and robotics</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>193</td>
  <td class=xl68 style='border-top:none;border-left:none'>647</td>
  <td class=xl69 style='border-top:none;border-left:none'>Dictionary-Guided
  Text Recognition for Smart Street Parking</td>
  <td class=xl69 style='border-top:none;border-left:none'>Deyang Zhong
  (University of Washington Tacoma); Jiayu Li (University of Washington ); Wei
  Cheng (University of Washington); Juhua Hu (University of Washington)*</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>194</td>
  <td class=xl68 style='border-top:none;border-left:none'>300</td>
  <td class=xl69 style='border-top:none;border-left:none'>Contrastive
  Consistent Representation Distillation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Shipeng Fu (Sichuan
  University )*; Haoran Yang (Sichuan University); Xiaomin Yang (Sichuan
  University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>195</td>
  <td class=xl68 style='border-top:none;border-left:none'>322</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Structure-guided
  Network for Tooth Alignment in 2D Photograph</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yulong Dou
  (Shanghaitech)*; Lanzhuju Mei (ShanghaiTech University); Zhiming Cui (HKU);
  Dinggang Shen (United Imaging Intelligence)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>196</td>
  <td class=xl68 style='border-top:none;border-left:none'>376</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adapting Generic
  Features to A Specific Task: A Large Discrepancy Knowledge Distillation for
  Image Anomaly Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Chenkai Zhang
  (Zhejiang University)*; Tianqi Du (Zhejiang University); Yueming Wang
  (Zhejiang University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>197</td>
  <td class=xl68 style='border-top:none;border-left:none'>385</td>
  <td class=xl69 style='border-top:none;border-left:none'>Personalized Fashion
  Recommendation via Deep Personality Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Dongmei Mo (The Hong
  Kong Polytechnic University)*; Xingxing Zou (Laboratory for Artificial
  Intelligence in Design, The Hong Kong Polytechnic University); Waikeung Wong
  (Institute of Textiles and Clothing, The Hong Kong Polytechnic University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>198</td>
  <td class=xl68 style='border-top:none;border-left:none'>480</td>
  <td class=xl69 style='border-top:none;border-left:none'>Comprehensive
  Quantitative Quality Assessment of Thermal Cut Sheet Edges using
  Convolutional Neural Networks</td>
  <td class=xl69 style='border-top:none;border-left:none'>Janek Stahl
  (Fraunhofer IPA)*; Marco Huber (University of Stuttgart); Andreas Frommknecht
  (Fraunhofer IPA)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>199</td>
  <td class=xl68 style='border-top:none;border-left:none'>614</td>
  <td class=xl69 style='border-top:none;border-left:none'>FRE: A Fast Method
  For Anomaly Detection And Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ibrahima Ndiour
  (Intel)*; Ergin U Genc (Intel); Nilesh A Ahuja (Intel); Omesh Tickoo (Intel)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Vision applications
  and systems</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl68 style='height:15.75pt;border-top:none;border-left:
  none'>200</td>
  <td class=xl68 style='border-top:none;border-left:none'>161</td>
  <td class=xl69 style='border-top:none;border-left:none'>Long Story Short: a
  Summarize-then-Search Method for Prompt-Based Long Video Question Answering</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jiwan Chung (Yonsei
  University)*; Youngjae Yu (Yonsei University)</td>
  <td class=xl69 style='border-top:none;border-left:none'>Visual reasoning and
  logical representation</td>
 </tr>
 <tr height=0 style='display:none'>
  <td width=101 style='width:76pt'></td>
  <td width=61 style='width:46pt'></td>
  <td width=61 style='width:46pt'></td>
  <td width=897 style='width:673pt'></td>
  <td width=2819 style='width:2114pt'></td>
  <td width=448 style='width:336pt'></td>
 </tr>

</table>

</div>



