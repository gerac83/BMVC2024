---
layout: default_sparse
title: Oral Presentations
permalink: /programme/oral/
index: 10
---

<style id="oral-12_13122_Styles">
<!--table
	{mso-displayed-decimal-separator:"\.";
	mso-displayed-thousand-separator:"\,";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
.style0
	{mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	white-space:nowrap;
	mso-rotate:0;
	mso-background-source:auto;
	mso-pattern:auto;
	color:black;
	font-size:10.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Arial;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	border:none;
	mso-protection:locked visible;
	mso-style-name:Normal;
	mso-style-id:0;}
td
	{mso-style-parent:style0;
	padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:10.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Arial;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{mso-style-parent:style0;
	color:white;
	font-size:11.0pt;
	font-weight:700;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:left;
	background:black;
	mso-pattern:black none;}
.xl66
	{mso-style-parent:style0;
	color:white;
	font-size:11.0pt;
	font-weight:700;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:left;
	border:.5pt solid black;
	background:black;
	mso-pattern:black none;}
.xl67
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:.5pt solid black;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl68
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border:.5pt solid black;}
.xl69
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	border:.5pt solid black;}
.xl70
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl71
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:none;
	border-bottom:.5pt solid black;
	border-left:.5pt solid black;}
.xl72
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:.5pt solid black;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl73
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl74
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:.5pt solid black;
	border-left:.5pt solid black;}
.xl75
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:.5pt solid black;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl76
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl77
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	border:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl78
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:none;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl79
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:.5pt solid black;
	border-bottom:.5pt solid black;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl80
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:.5pt solid black;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl81
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl82
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:none;
	border-right:none;
	border-bottom:.5pt solid black;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl83
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:right;
	border-top:.5pt solid black;
	border-right:.5pt solid black;
	border-bottom:.5pt solid black;
	border-left:none;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
.xl84
	{mso-style-parent:style0;
	color:windowtext;
	border-top:none;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl85
	{mso-style-parent:style0;
	color:windowtext;
	border-top:none;
	border-right:none;
	border-bottom:.5pt solid black;
	border-left:.5pt solid black;}
.xl86
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:center;
	vertical-align:middle;
	border-top:.5pt solid black;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;}
.xl87
	{mso-style-parent:style0;
	font-size:11.0pt;
	font-family:Calibri;
	mso-generic-font-family:auto;
	mso-font-charset:0;
	text-align:center;
	vertical-align:middle;
	border-top:.5pt solid black;
	border-right:none;
	border-bottom:none;
	border-left:.5pt solid black;
	background:#EFEFEF;
	mso-pattern:#EFEFEF none;}
-->
</style>

<div id="oral-12_13122" align=center x:publishsource="Excel">

<table border=0 cellpadding=0 cellspacing=0 width=3737 style='border-collapse:
 collapse;table-layout:fixed;width:2803pt'>
 <col width=77 style='mso-width-source:userset;mso-width-alt:2474;width:58pt'>
 <col width=88 style='mso-width-source:userset;mso-width-alt:2816;width:66pt'>
 <col width=61 span=2 style='mso-width-source:userset;mso-width-alt:1962;
 width:46pt'>
 <col width=264 style='mso-width-source:userset;mso-width-alt:8448;width:198pt'>
 <col width=955 style='mso-width-source:userset;mso-width-alt:30549;width:716pt'>
 <col width=2231 style='mso-width-source:userset;mso-width-alt:71381;
 width:1673pt'>
 <col width=101 span=4 style='width:76pt'>
 <col width=897 style='mso-width-source:userset;mso-width-alt:28714;width:673pt'>
 <col width=448 style='mso-width-source:userset;mso-width-alt:14336;width:336pt'>
 <col width=101 span=8 style='width:76pt'>
 <col width=145 style='mso-width-source:userset;mso-width-alt:4650;width:109pt'>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl65 width=77 style='height:15.75pt;width:58pt'>Day</td>
  <td class=xl65 width=88 style='width:66pt'>Time</td>
  <td class=xl66 width=61 style='width:46pt'>Number</td>
  <td class=xl66 width=61 style='border-left:none;width:46pt'>Paper ID</td>
  <td class=xl66 width=264 style='border-left:none;width:198pt'>Session Topic</td>
  <td class=xl66 width=955 style='border-left:none;width:716pt'>Title</td>
  <td class=xl66 width=2231 style='border-left:none;width:1673pt'>Authors</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=18 height=360 class=xl86 style='border-bottom:.5pt solid black;
  height:283.5pt'>Monday</td>
  <td class=xl67>11:45-12:00</td>
  <td class=xl68 style='border-top:none'>1</td>
  <td class=xl68 style='border-top:none;border-left:none'>871</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>On the Lipschitz
  Constant of Deep Networks and Double Descent</td>
  <td class=xl69 style='border-top:none;border-left:none'>Matteo Gamba (KTH)*;
  Hossein Azizpour (KTH (Royal Institute of Technology)); Marten Bjorkman (KTH)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl70 style='height:15.75pt'>12:00-12:15</td>
  <td class=xl68 style='border-top:none'>2</td>
  <td class=xl68 style='border-top:none;border-left:none'>265</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>A Multi-step Fusion
  Network Based on Environmental Knowledge Graph for Camouflaged Object
  Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Zheng Wang (Tianjin
  University)*; Wenjun Huang (Tianjin University); Ruoxun Su (Tianjin
  University); Xinyu Yan (Tianjin University); Meijun Sun (Tianjin University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl70 style='height:15.75pt'>12:15-12:30</td>
  <td class=xl68 style='border-top:none'>3</td>
  <td class=xl68 style='border-top:none;border-left:none'>437</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>Maturity-Aware Active
  Learning for Semantic Segmentation with Hierarchically-Adaptive Sample
  Assessment</td>
  <td class=xl69 style='border-top:none;border-left:none'>Amirsaeed Yazdani
  (Pennsylvania State University)*; Xuelu Li (Amazon); Vishal Monga (The
  Pennsylvania State University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl70 style='height:15.75pt'>12:30-12:45</td>
  <td class=xl68 style='border-top:none'>4</td>
  <td class=xl68 style='border-top:none;border-left:none'>912</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>Group
  Orthogonalization Regularization for Vision Models Adaptation and Robustness</td>
  <td class=xl69 style='border-top:none;border-left:none'>Yoav Kurtz (Tel Aviv
  University); Noga Bar (Tel Aviv University); Raja Giryes (Tel Aviv
  University)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl71 style='height:15.75pt'>12:45-13:00</td>
  <td class=xl68 style='border-top:none'>5</td>
  <td class=xl68 style='border-top:none;border-left:none'>276</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>Attentive Contractive
  Flow with Lipschitz Constrained Self-Attention</td>
  <td class=xl69 style='border-top:none;border-left:none'>Avideep Mukherjee
  (Indian Institute of Technology Kanpur)*; Badri N Patro (KU Leuven); Vinay
  Namboodiri (University of Bath)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl72 style='height:15.75pt;border-top:none'>14:00-14:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>6</td>
  <td class=xl68 style='border-top:none;border-left:none'>497</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>EDeNN: Event Decay
  Neural Networks for low latency vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Celyn Walters
  (University of Surrey); Simon Hadfield (University of Surrey)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>14:15-14:30</td>
  <td class=xl68 style='border-top:none;border-left:none'>7</td>
  <td class=xl68 style='border-top:none;border-left:none'>669</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>SRBGCN: Tangent
  space-Free Lorentz Transformations for Graph Feature Learning</td>
  <td class=xl69 style='border-top:none;border-left:none'>Abdelrahman Mostafa
  (University of Oulu)*; Wei Peng (Stanford University); Guoying Zhao
  (University of Oulu)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>14:30-14:45</td>
  <td class=xl68 style='border-top:none;border-left:none'>8</td>
  <td class=xl68 style='border-top:none;border-left:none'>601</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>Overcoming
  Degradation Imbalance for Consistent Image Dehazing</td>
  <td class=xl69 style='border-top:none;border-left:none'>Pranjay Shyam
  (Faurecia IRYStec)*; Hyunjin Yoo (Faurecia IRYStec)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>14:45-15:00</td>
  <td class=xl68 style='border-top:none;border-left:none'>9</td>
  <td class=xl68 style='border-top:none;border-left:none'>829</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>PseudoCal: Towards
  Initialisation-Free Deep Learning-Based Camera-LiDAR Self-Calibration</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mathieu Cocheteux
  (Université de Technologie de Compiègne)*; Franck Davoine (Heudiasyc - CNRS -
  Université de technologie de Compiègne); Julien Moreau (UTC, Heudiasyc-SyRI)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl74 style='height:15.75pt'>15:00-15:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>10</td>
  <td class=xl68 style='border-top:none;border-left:none'>721</td>
  <td class=xl69 style='border-top:none;border-left:none'>Architectures and
  Techniques</td>
  <td class=xl69 style='border-top:none;border-left:none'>Convolution kernel
  adaptation to calibrated fisheye</td>
  <td class=xl69 style='border-top:none;border-left:none'>Bruno Berenguel-Baeta
  (Universidad de Zaragoza)*; Maria Santos-Villafranca (Universidad de
  Zaragoza); Jesus Bermudez-Cameo (Universidad de Zaragoza); Alejandro Perez
  Yus (Universidad de Zaragoza); Josechu Guerrero (Universidad de Zaragoza)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl72 style='height:15.75pt;border-top:none'>15:45-16:00</td>
  <td class=xl68 style='border-top:none;border-left:none'>11</td>
  <td class=xl68 style='border-top:none;border-left:none'>901</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>On-Site Adaptation
  for Monocular Depth Estimation with a Static Camera</td>
  <td class=xl69 style='border-top:none;border-left:none'>Huan Li (Bologna
  University)*; Matteo Poggi (University of Bologna); Fabio Tosi (University of
  Bologna); Stefano Mattoccia (University of Bologna)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>16:00-16:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>12</td>
  <td class=xl68 style='border-top:none;border-left:none'>823</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Improved Photometric
  Stereo through Efficient and Differentiable Shadow Estimation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Po-Hung Yeh (National
  Taiwan University); Pei-Yuan Wu (National Taiwan University); Jun-Cheng Chen
  (Academia Sinica)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>15:15-16:30</td>
  <td class=xl68 style='border-top:none;border-left:none'>13</td>
  <td class=xl68 style='border-top:none;border-left:none'>405</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Breathing New Life
  into 3D Assets with Generative Repainting</td>
  <td class=xl69 style='border-top:none;border-left:none'>Tianfu Wang (ETH
  Zurich); Menelaos Kanakis (ETH Zurich); Konrad Schindler (ETH Zurich); Luc
  Van Gool (ETH Zurich); Anton Obukhov (ETH Zurich)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl74 style='height:15.75pt'>16:30-16:45</td>
  <td class=xl68 style='border-top:none;border-left:none'>14</td>
  <td class=xl68 style='border-top:none;border-left:none'>14</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>A-Scan2BIM: Assistive
  Scan to Building Information Modeling</td>
  <td class=xl69 style='border-top:none;border-left:none'>Weilian Song (Simon
  Fraser University)*; Jieliang Luo (Autodesk Research); Dale Zhao (Autodesk
  Research); Yan Fu (Autodesk Research); Chin-Yi Cheng (Google Research);
  Yasutaka Furukawa (Simon Fraser University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl72 style='height:15.75pt;border-top:none'>17:00-17:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>15</td>
  <td class=xl68 style='border-top:none;border-left:none'>571</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Exploiting Multiple
  Priors for Neural 3D Indoor Reconstruction</td>
  <td class=xl69 style='border-top:none;border-left:none'>Federico Lincetto
  (University of Padova)*; Gianluca Agresti (Sony Europe B.V.); Mattia Rossi
  (SONY Europe B.V.); Pietro Zanuttigh (University of Padova)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>17:15-17:30</td>
  <td class=xl68 style='border-top:none;border-left:none'>16</td>
  <td class=xl68 style='border-top:none;border-left:none'>339</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Structured Knowledge
  Distillation Towards Efficient Multi-View 3D Object Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Linfeng Zhang
  (Tsinghua University )*; Yukang Shi (Xi’an Jiaotong University); Ke Wang (UNC
  Chapel Hill); Zhipeng Zhang (DiDi); Hung-Shuo Tai (Didi Autonomous Drive);
  Yuan He (KargoBot); Kaisheng Ma (Tsinghua University )</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>17:30-17:45</td>
  <td class=xl68 style='border-top:none;border-left:none'>17</td>
  <td class=xl68 style='border-top:none;border-left:none'>305</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Learnable Geometry
  and Connectivity Modelling of BIM Objects</td>
  <td class=xl69 style='border-top:none;border-left:none'>Haritha Jayasinghe
  (University of Cambridge)*; Ioannis Brilakis (University of Cambridge)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl74 style='height:15.75pt'>17:45-18:00</td>
  <td class=xl68 style='border-top:none;border-left:none'>18</td>
  <td class=xl68 style='border-top:none;border-left:none'>438</td>
  <td class=xl69 style='border-top:none;border-left:none'>3D Analysis</td>
  <td class=xl69 style='border-top:none;border-left:none'>Score-PA: Score-based
  3D Part Assembly</td>
  <td class=xl69 style='border-top:none;border-left:none'>Junfeng Cheng
  (Imperial College London); Mingdong Wu (Peking University); Ruiyuan Zhang
  (zhejiang university); Guanqi Zhan (University of Oxford); Chao Wu (Zhejiang
  University); Hao Dong (Peking University)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=17 height=340 class=xl87 style='border-bottom:.5pt solid black;
  height:267.75pt;border-top:none'>Tuesday</td>
  <td class=xl75 style='border-top:none'>10:00-10:15</td>
  <td class=xl76 style='border-top:none;border-left:none'>19</td>
  <td class=xl76 style='border-top:none;border-left:none'>187</td>
  <td class=xl77 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
  <td class=xl77 style='border-top:none;border-left:none'>Can Deep Networks be
  Highly Performant, Efficient and Robust simultaneously?</td>
  <td class=xl77 style='border-top:none;border-left:none'>Madan Ravi Ganesh
  (BCAI)*; Salimeh Yasaei Sekeh (University of Maine); Jason J Corso
  (University of Michigan)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>10:15-10:30</td>
  <td class=xl76 style='border-top:none;border-left:none'>20</td>
  <td class=xl76 style='border-top:none;border-left:none'>290</td>
  <td class=xl77 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
  <td class=xl77 style='border-top:none;border-left:none'>Highly Efficient SNNs
  for High-speed Object Detection</td>
  <td class=xl77 style='border-top:none;border-left:none'>Nemin Qiu (Beijing
  University of Posts and Telecommunications)*; zhiguo li (Peking University);
  Yuan Li (Peking University); Chuang Zhu (Beijing University of Posts and
  Telecommunications )</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>10:30-10:45</td>
  <td class=xl76 style='border-top:none;border-left:none'>21</td>
  <td class=xl76 style='border-top:none;border-left:none'>832</td>
  <td class=xl77 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
  <td class=xl77 style='border-top:none;border-left:none'>Feather: An Elegant
  Solution to Effective DNN Sparsification</td>
  <td class=xl77 style='border-top:none;border-left:none'>Athanasios Glentis
  Georgoulakis (National Technical University of Athens)*; George Retsinas
  (National Technical University of Athens); Petros Maragos (National Technical
  University of Athens)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl79 style='height:15.75pt'>10:45-11:00</td>
  <td class=xl76 style='border-top:none;border-left:none'>22</td>
  <td class=xl76 style='border-top:none;border-left:none'>311</td>
  <td class=xl77 style='border-top:none;border-left:none'>Efficient and
  scalable vision</td>
  <td class=xl77 style='border-top:none;border-left:none'>RepQ: Generalizing
  Quantization-Aware Training for Re-Parametrized Architectures</td>
  <td class=xl77 style='border-top:none;border-left:none'>Anastasiia Prutianova
  (Huawei)*; Alexey Zaytsev (Skoltech); Chung-Kuei Lee (Huawei); Fengyu Sun
  (Huawei); Ivan Koryakovskiy (Huawei Technologies Co., Ltd.)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl80 style='height:15.75pt;border-top:none'>11:45-12:00</td>
  <td class=xl76 style='border-top:none'>23</td>
  <td class=xl76 style='border-top:none;border-left:none'>53</td>
  <td class=xl77 style='border-top:none;border-left:none'>Explainable AI &amp;
  Representation Learning</td>
  <td class=xl77 style='border-top:none;border-left:none'>Unsupervised Hashing
  with Similarity Distribution Calibration</td>
  <td class=xl77 style='border-top:none;border-left:none'>Kam Woh Ng
  (University of Surrey)*; Xiatian Zhu (University of Surrey); Jiun Tian Hoe
  (Nanyang Technological University); Chee Seng Chan (University of Malaya);
  Tianyu Zhang (Geek Plus); Yi-Zhe Song (University of Surrey); Tao Xiang
  (University of Surrey)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:00-12:15</td>
  <td class=xl76 style='border-top:none'>24</td>
  <td class=xl76 style='border-top:none;border-left:none'>70</td>
  <td class=xl77 style='border-top:none;border-left:none'>Explainable AI &amp;
  Representation Learning</td>
  <td class=xl77 style='border-top:none;border-left:none'>Diversifying the
  High-level Features for better Adversarial Transferability</td>
  <td class=xl77 style='border-top:none;border-left:none'>Zhiyuan Wang
  (Huazhong University of Science and Technology); Zeliang Zhang (University of
  Rochester); Siyuan Liang (Chinese Academy of Sciences); Xiaosen Wang
  (Huazhong University of Science and Technology)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:15-12:30</td>
  <td class=xl76 style='border-top:none'>25</td>
  <td class=xl76 style='border-top:none;border-left:none'>685</td>
  <td class=xl77 style='border-top:none;border-left:none'>Explainable AI &amp;
  Representation Learning</td>
  <td class=xl77 style='border-top:none;border-left:none'>Protecting Publicly
  Available Data With Machine Learning Shortcuts</td>
  <td class=xl77 style='border-top:none;border-left:none'>Nicolas M Müller
  (Fraunhofer AISEC)*; Maximilian Burgert (TU Munich); Pascal Debus (Fraunhofer
  AISEC); Jennifer Williams (University of Southampton); Philip Sperl
  (Fraunhofer AISEC); Konstantin Böttinger (Fraunhofer AISEC)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:30-12:45</td>
  <td class=xl76 style='border-top:none'>26</td>
  <td class=xl76 style='border-top:none;border-left:none'>771</td>
  <td class=xl77 style='border-top:none;border-left:none'>Explainable AI &amp;
  Representation Learning</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision Transformers
  are Inherently Saliency Learners</td>
  <td class=xl77 style='border-top:none;border-left:none'>Yasser Abdelaziz
  DAHOU DJILALI (Dublin City UNIVERISTY )*; Kevin McGuinness (DCU); Noel O
  Connor (Home)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl82 style='height:15.75pt'>12:45-13:00</td>
  <td class=xl76 style='border-top:none'>27</td>
  <td class=xl76 style='border-top:none;border-left:none'>578</td>
  <td class=xl77 style='border-top:none;border-left:none'>Explainable AI &amp;
  Representation Learning</td>
  <td class=xl77 style='border-top:none;border-left:none'>H-NeXt: The next step
  towards roto-translation invariant networks</td>
  <td class=xl77 style='border-top:none;border-left:none'>Tomáš Karella
  (Institute of Information Theory and Automation, Czech Academy of Sciences)*;
  Filip Šroubek (Institute of Information Theory and Automation, Czech Academy
  of Sciences); Jan Blažek (Institute of Information Theory and Automation,
  Czech Academy of Sciences); Jan Flusser (UTIA, Czech Academy of Sciences);
  Václav Košík (UTIA, Czech Academy of Sciences)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl75 style='height:15.75pt;border-top:none'>15:45-16:00</td>
  <td class=xl76 style='border-top:none;border-left:none'>28</td>
  <td class=xl76 style='border-top:none;border-left:none'>45</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>A Critical Robustness
  Evaluation for Referring Expression Comprehension Methods</td>
  <td class=xl77 style='border-top:none;border-left:none'>zhipeng zhang
  (Northwestern Polytechnical University); Zhimin Wei (Northwestern
  Polytechnical University); Peng Wang (Northwestern Polytechnical University)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>16:00-16:15</td>
  <td class=xl76 style='border-top:none;border-left:none'>29</td>
  <td class=xl76 style='border-top:none;border-left:none'>377</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>Describe Your Facial
  Expressions by Linking Image Encoders and Large Language Models</td>
  <td class=xl77 style='border-top:none;border-left:none'>Yujian Yuan
  (Institute of Computing Technology, Chinese Academy of Sciences; University
  of Chinese Academy of Sciences); Jiabei Zeng (Institute of Computing
  Technology, Chinese Academy of Sciences)*; Shiguang Shan (Institute of
  Computing Technology, Chinese Academy of Sciences)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>15:15-16:30</td>
  <td class=xl76 style='border-top:none;border-left:none'>30</td>
  <td class=xl76 style='border-top:none;border-left:none'>722</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>Spatio-Temporal Graph
  Diffusion for Text-Driven Human Motion Generation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Chang Liu (University
  of Trento)*; Mengyi Zhao (Beihang University); Bin Ren (University of
  Trento); Mengyuan Liu (Peking University, Shenzhen Graduate School); Nicu
  Sebe (University of Trento)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl79 style='height:15.75pt'>16:30-16:45</td>
  <td class=xl76 style='border-top:none;border-left:none'>31</td>
  <td class=xl76 style='border-top:none;border-left:none'>366</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>Divide &amp; Bind
  Your Attention for Improved Generative Semantic Nursing</td>
  <td class=xl77 style='border-top:none;border-left:none'>Yumeng Li (Bosch
  Center for Artificial Intelligence)*; Margret Keuper (University of Siegen,
  Max Planck Institute for Informatics); Dan Zhang (Bosch Center for Artificial
  Intelligence); Anna Khoreva (Bosch Center for Artificial Intelligence)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>17:00-17:15</td>
  <td class=xl76 style='border-top:none'>32</td>
  <td class=xl76 style='border-top:none;border-left:none'>748</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>Multi-CLIP:
  Contrastive Vision-Language Pre-training for Question Answering tasks in 3D
  Scenes</td>
  <td class=xl77 style='border-top:none;border-left:none'>Alexandros Delitzas
  (ETH Zurich)*; Maria Parelli (ETH Zurich); Nikolas Hars (ETH Zurich);
  Georgios Vlassis (ETH Zurich); Sotirios-Konstantinos Anagnostidis (ETH
  Zurich); Gregor Bachmann (ETH Zurich); Thomas Hofmann (ETH Zurich)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>17:15-17:30</td>
  <td class=xl76 style='border-top:none'>33</td>
  <td class=xl76 style='border-top:none;border-left:none'>670</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>DisCLIP:
  Open-Vocabulary Referring Expression Generation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Lior Bracha (Bar Ilan
  University)*; Eitan Shaar (bar Ilan University); Aviv Shamsian (Bar Ilan
  University); Ethan Fetaya (Bar Ilan University); Gal Chechik (NVIDIA)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>17:30-17:45</td>
  <td class=xl76 style='border-top:none'>34</td>
  <td class=xl76 style='border-top:none;border-left:none'>581</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>Video-adverb
  retrieval with compositional adverb-action embeddings</td>
  <td class=xl77 style='border-top:none;border-left:none'>Thomas Hummel
  (University of Tübingen)*; A. Sophia Koepke (University of Tübingen);
  Otniel-Bogdan Mercea (University of Tübingen); Zeynep Akata (University of
  Tübingen)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>17:45-18:00</td>
  <td class=xl76 style='border-top:none'>35</td>
  <td class=xl76 style='border-top:none;border-left:none'>429</td>
  <td class=xl77 style='border-top:none;border-left:none'>Vision and language</td>
  <td class=xl77 style='border-top:none;border-left:none'>Zero-Shot Video
  Captioning by Evolving Pseudo-tokens</td>
  <td class=xl77 style='border-top:none;border-left:none'>Yoad Tewel (Tel-Aviv
  University)*; Yoav Shalev (Tel Aviv University); Roy Nadler (Tel Aviv
  University); Idan Schwartz (Technion); Lior Wolf (Tel Aviv University,
  Israel)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=17 height=340 class=xl86 style='border-bottom:.5pt solid black;
  height:267.75pt;border-top:none'>Wednesday</td>
  <td class=xl72>10:00-10:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>36</td>
  <td class=xl68 style='border-top:none;border-left:none'>114</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
  <td class=xl69 style='border-top:none;border-left:none'>Attributes-Aware
  Network for Temporal Action Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Rui Dai (INRIA)*;
  Srijan Das (University of North Carolina at Charlotte); Michael S Ryoo (Stony
  Brook/Google); Francois Bremond (Inria Sophia Antipolis, France)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>10:15-10:30</td>
  <td class=xl68 style='border-top:none;border-left:none'>37</td>
  <td class=xl68 style='border-top:none;border-left:none'>179</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
  <td class=xl69 style='border-top:none;border-left:none'>Boost Video Frame
  Interpolation via Motion Adaptation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Haoning Wu (Shanghai
  Jiao Tong University); Xiaoyun Zhang (Shanghai Jiao Tong University)*; Weidi
  Xie (Shanghai Jiao Tong University); Ya Zhang (Cooperative Medianet
  Innovation Center, Shang hai Jiao Tong University); Yan-Feng Wang
  (Cooperative medianet innovation center of Shanghai Jiao Tong University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>10:30-10:45</td>
  <td class=xl68 style='border-top:none;border-left:none'>38</td>
  <td class=xl68 style='border-top:none;border-left:none'>589</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
  <td class=xl69 style='border-top:none;border-left:none'>Staged Contact-Aware
  Global Human Motion Forecasting</td>
  <td class=xl69 style='border-top:none;border-left:none'>Luca Scofano
  (Sapienza University of Rome); Alessio Sampieri (Sapienza University)*;
  Elisabeth Schiele (Technische Universität München); Edoardo De Matteis
  (Sapienza University of Rome); Laura Leal-Taixé (NVIDIA); Fabio Galasso
  (Sapienza University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl74 style='height:15.75pt'>10:45-11:00</td>
  <td class=xl68 style='border-top:none;border-left:none'>39</td>
  <td class=xl68 style='border-top:none;border-left:none'>317</td>
  <td class=xl69 style='border-top:none;border-left:none'>Action and event
  understanding</td>
  <td class=xl69 style='border-top:none;border-left:none'>Spherical Vision
  Transformer for 360° Video Saliency Prediction</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mert Cokelek (Koç
  University)*; Nevrez Imamoglu (AIST); Cagri Ozcinar (Samsung); Erkut Erdem
  (Hacettepe University); Aykut Erdem (Koc University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl67 style='height:15.75pt;border-top:none'>11:45-12:00</td>
  <td class=xl68 style='border-top:none'>40</td>
  <td class=xl68 style='border-top:none;border-left:none'>402</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition/Identification/Detetcion</td>
  <td class=xl69 style='border-top:none;border-left:none'>Revisiting the
  Encoding of Satellite Image Time Series</td>
  <td class=xl69 style='border-top:none;border-left:none'>Xin Cai (Ulster
  University)*; Yaxin Bi (Ulster University); Peter Nicholl (Ulster
  University); Roy Sterritt (Ulster University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl70 style='height:15.75pt'>12:00-12:15</td>
  <td class=xl68 style='border-top:none'>41</td>
  <td class=xl68 style='border-top:none;border-left:none'>10</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition/Identification/Detetcion</td>
  <td class=xl69 style='border-top:none;border-left:none'>Improving
  Out-of-Distribution Detection Performance using Synthetic Outlier Exposure
  Generated by Visual Foundation Models</td>
  <td class=xl69 style='border-top:none;border-left:none'>Gitaek Kwon (VUNO
  Inc.); Jaeyoung Kim (VUNO Inc.)*; Hong-Jun Choi (VUNO Inc.); Byung-Moo Yoon
  (Gachon University); Sungchul Choi (Pukyong National University); Kyu-Hwan
  Jung (Sungkyunkwan University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl70 style='height:15.75pt'>12:15-12:30</td>
  <td class=xl68 style='border-top:none'>42</td>
  <td class=xl68 style='border-top:none;border-left:none'>82</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition/Identification/Detetcion</td>
  <td class=xl69 style='border-top:none;border-left:none'>Object-Centric
  Multi-Task Learning for Human Instances</td>
  <td class=xl69 style='border-top:none;border-left:none'>Hyeongseok Son
  (Samsung Advanced Institute of Technology)*; Sangil Jung (Samsung); Solae Lee
  (Samsung Advanced Institute of Technology); Seongeun Kim (Samsung); Seung-In
  Park (SAIT); ByungIn Yoo (Samsung Advanced Institute of Technology)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl70 style='height:15.75pt'>12:30-12:45</td>
  <td class=xl68 style='border-top:none'>43</td>
  <td class=xl68 style='border-top:none;border-left:none'>197</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition/Identification/Detetcion</td>
  <td class=xl69 style='border-top:none;border-left:none'>Domain-Sum Feature
  Transformation For Multi-Target Domain Adaptation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Takumi Kobayashi
  (National Institute of Advanced Industrial Science and Technology)*; Lincon
  Souza (National Institute of Advanced Industrial Science and Technology
  (AIST)); Kazuhiro Fukui (University of Tsukuba)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl71 style='height:15.75pt'>12:45-13:00</td>
  <td class=xl68 style='border-top:none'>44</td>
  <td class=xl68 style='border-top:none;border-left:none'>192</td>
  <td class=xl69 style='border-top:none;border-left:none'>Recognition/Identification/Detetcion</td>
  <td class=xl69 style='border-top:none;border-left:none'>MILA: Memory-Based
  Instance-Level Adaptation for Cross-Domain Object Detection</td>
  <td class=xl69 style='border-top:none;border-left:none'>Onkar Krishna
  (Hitachi Ltd.)*; Hiroki Ohashi (Hitachi Ltd); Saptarshi Sinha (University of
  Bristol)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl72 style='height:15.75pt;border-top:none'>15:30-15:45</td>
  <td class=xl68 style='border-top:none;border-left:none'>45</td>
  <td class=xl68 style='border-top:none;border-left:none'>617</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Learning Anatomically
  Consistent Embedding for Chest Radiography</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ziyu Zhou (Shanghai
  Jiao Tong University); Haozhe Luo ( Arizona State University, USA ); Jiaxuan
  Pang (Arizona State University); xiaowei ding (Shanghai Jiao Tong
  University); Michael Gotway (Mayo Clinic); Jianming Liang (Arizona State
  University, USA)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>15:45-16:00</td>
  <td class=xl68 style='border-top:none;border-left:none'>46</td>
  <td class=xl68 style='border-top:none;border-left:none'>754</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Single-Landmark vs.
  Multi-Landmark Deep Learning Approaches to Brain MRI Landmarking: a Case
  Study with Healthy Controls and Down Syndrome Individuals</td>
  <td class=xl69 style='border-top:none;border-left:none'>Jordi Malé (La Salle
  - Ramon Llull University)*; Yann Heuzé (CNRS, Univ. Bordeaux, MC, PACEA,
  UMR5199); Juan Fortea (Hospital of Sant Pau); Neus Martinez Abadias
  (Universitat de Barcelona); Xavier Sevillano (La Salle - Universitat Ramon
  Llull)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>16:00-16:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>47</td>
  <td class=xl68 style='border-top:none;border-left:none'>482</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>BiUNet: Towards More
  Effective UNet with Bi-Level Routing Attention</td>
  <td class=xl69 style='border-top:none;border-left:none'>Kun Dong (University
  of Chinese Academy of Sciences); Jian Xue (University of Chinese Academy of
  Sciences); Xing Lan (University of Chinese Academy of Sciences); Ke Lu
  (University of Chinese Academy of Sciences)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>16:15-16:30</td>
  <td class=xl68 style='border-top:none;border-left:none'>48</td>
  <td class=xl68 style='border-top:none;border-left:none'>575</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Dual-Query Multiple
  Instance Learning for Dynamic Meta-Embedding based Tumor Classification</td>
  <td class=xl69 style='border-top:none;border-left:none'>Simon
  Holdenried-Krafft (University of Tübingen)*; Peter Somers (University of
  Tübingen); Ivonne Montes-Mojarro (University Hospital of Tübingen); Diana
  Silimon (University Hospital of Tübingen); Cristina Tarín (University of
  Stuttgart); Falko Fend (University Hospital of Tübingen); Hendrik P. A.
  Lensch (University of Tübingen)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>16:30-16:45</td>
  <td class=xl68 style='border-top:none;border-left:none'>49</td>
  <td class=xl68 style='border-top:none;border-left:none'>806</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Adaptation of
  Distinct Semantics for Uncertain Areas in Polyp Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Quang Vinh Nguyen
  (Chonnam National University)*; Van Thong Huynh (Chonnam National
  University); Soo-Hyung Kim (Chonnam National University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>16:45-17:00</td>
  <td class=xl68 style='border-top:none;border-left:none'>50</td>
  <td class=xl68 style='border-top:none;border-left:none'>152</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Primitive Geometry
  Segment Pre-training for 3D Medical Image Segmentation</td>
  <td class=xl69 style='border-top:none;border-left:none'>Ryu Tadokoro (Tohoku
  University)*; Ryosuke Yamada (University of Tsukuba, National Institute of
  Advanced Industrial Science and Technology (AIST)); Kodai Nakashima
  (CyberAgent, Univ. of Tsukuba, AIST); Ryo Nakamura (Fukuoka University,
  National Institute of Advanced Industrial Science and Technology (AIST));
  Hirokatsu Kataoka (National Institute of Advanced Industrial Science and
  Technology (AIST))</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl73 style='height:15.75pt'>17:00-17:15</td>
  <td class=xl68 style='border-top:none;border-left:none'>51</td>
  <td class=xl68 style='border-top:none;border-left:none'>881</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>Rethinking Transfer
  Learning for Medical Image Classification</td>
  <td class=xl69 style='border-top:none;border-left:none'>Le Peng (University
  of Minnesota)*; Hengyue Liang (University of Minnesota); Gaoxiang Luo
  (University of Minnesota); Taihui Li (University of Minnesota); Ju Sun
  (University of Minnesota)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl74 style='height:15.75pt'>17:15-17:30</td>
  <td class=xl68 style='border-top:none;border-left:none'>52</td>
  <td class=xl68 style='border-top:none;border-left:none'>730</td>
  <td class=xl69 style='border-top:none;border-left:none'>Medical and
  biological vision</td>
  <td class=xl69 style='border-top:none;border-left:none'>SA2-Net: Scale-aware
  Attention Network for Cell Segmentation and Beyond</td>
  <td class=xl69 style='border-top:none;border-left:none'>Mustansar Fiaz
  (MBZUAI)*; Moein Heidari (Iran University of Science and Technology); Rao
  Muhammad Anwer (MBZUAI/AALTO); Hisham Cholakkal (MBZUAI)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td rowspan=15 height=300 class=xl87 style='border-bottom:.5pt solid black;
  height:236.25pt;border-top:none'>Thursday</td>
  <td class=xl75 style='border-top:none'>10:00-10:15</td>
  <td class=xl76 style='border-top:none;border-left:none'>53</td>
  <td class=xl76 style='border-top:none;border-left:none'>193</td>
  <td class=xl77 style='border-top:none;border-left:none'>Human/Object Pose
  Estimation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Functional Hand Type
  Prior for 3D Hand Pose Estimation and Action Recognition from Egocentric View
  Monocular Videos</td>
  <td class=xl77 style='border-top:none;border-left:none'>WONSEOK ROH (Korea
  University); Seung Hyun Lee (Korea University); Won Jeong Ryoo (Korea
  University); Gyeongrok Oh (Korea University); Jakyung Lee (Korea University);
  Sooyeon Hwang (Korea University Sejong); Hyung-gun Chi (Purdue University);
  Sangpil Kim (Korea University)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>10:15-10:30</td>
  <td class=xl76 style='border-top:none;border-left:none'>54</td>
  <td class=xl76 style='border-top:none;border-left:none'>609</td>
  <td class=xl77 style='border-top:none;border-left:none'>Human/Object Pose
  Estimation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Cross-attention
  Masked Auto-Encoder for Human 3D Motion Infilling and Denoising</td>
  <td class=xl77 style='border-top:none;border-left:none'>David Björkstrand
  (KTH Royal Institute of Technology / Tracab)*; Josephine Sullivan (KTH Royal
  Institute of Technology); Lars M C Bretzner (Tracab AB); Gareth Loy (TRACAB);
  Tiesheng Wang (Tracab)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>10:30-10:45</td>
  <td class=xl76 style='border-top:none;border-left:none'>55</td>
  <td class=xl76 style='border-top:none;border-left:none'>167</td>
  <td class=xl77 style='border-top:none;border-left:none'>Human/Object Pose
  Estimation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Efficient Vision
  Transformer for Human Pose Estimation via Patch Selection</td>
  <td class=xl77 style='border-top:none;border-left:none'>Kaleab A Kinfu (Johns
  Hopkins University)*; Rene Vidal (Johns Hopkins University, USA)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl79 style='height:15.75pt'>10:45-11:00</td>
  <td class=xl76 style='border-top:none;border-left:none'>56</td>
  <td class=xl76 style='border-top:none;border-left:none'>543</td>
  <td class=xl77 style='border-top:none;border-left:none'>Human/Object Pose
  Estimation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Robust and Efficient
  Edge-guided Pose Estimation with Resolution-conditioned NeRF</td>
  <td class=xl77 style='border-top:none;border-left:none'>Liesbeth Claessens
  (ETH Zurich)*; Fabian Manhardt (Google); Ricardo Martin-Brualla (Google);
  Roland Siegwart (ETH Zürich, Autonomous Systems Lab); Cesar Cadena Lerma (ETH
  Zurich); Federico Tombari (Google)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl80 style='height:15.75pt;border-top:none'>11:45-12:00</td>
  <td class=xl76 style='border-top:none'>57</td>
  <td class=xl76 style='border-top:none;border-left:none'>329</td>
  <td class=xl77 style='border-top:none;border-left:none'>Transfer, low-shot
  learning &amp; Segmentation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Maskomaly: Zero-Shot
  Mask Anomaly Segmentation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Jan Ackermann (ETH
  Zurich)*; Christos Sakaridis (ETH Zurich); Fisher Yu (ETH Zurich)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:00-12:15</td>
  <td class=xl76 style='border-top:none'>58</td>
  <td class=xl76 style='border-top:none;border-left:none'>606</td>
  <td class=xl77 style='border-top:none;border-left:none'>Transfer, low-shot
  learning &amp; Segmentation</td>
  <td class=xl77 style='border-top:none;border-left:none'>STARS: Zero-shot
  Sim-to-Real Transfer for Segmentation of Shipwrecks in Sonar Imagery</td>
  <td class=xl77 style='border-top:none;border-left:none'>Advaith V Sethuraman
  (University of Michigan )*; Katherine A Skinner (University of Michigan)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:15-12:30</td>
  <td class=xl76 style='border-top:none'>59</td>
  <td class=xl76 style='border-top:none;border-left:none'>544</td>
  <td class=xl77 style='border-top:none;border-left:none'>Transfer, low-shot
  learning &amp; Segmentation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Re-Degradation and
  Contrastive Learning for Zero-shot Underwater Image Restoration</td>
  <td class=xl77 style='border-top:none;border-left:none'>Nisha Varghese (IIT
  Madras)*; Rajagopalan N Ambasamudram (Indian Institute of Technology Madras)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:30-12:45</td>
  <td class=xl76 style='border-top:none'>60</td>
  <td class=xl76 style='border-top:none;border-left:none'>566</td>
  <td class=xl77 style='border-top:none;border-left:none'>Transfer, low-shot
  learning &amp; Segmentation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Polarimetric Imaging
  for Perception</td>
  <td class=xl77 style='border-top:none;border-left:none'>Michael Baltaxe
  (General Motors)*; Tomer Pe'er (General Motors); Dan Levi (General Motors)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl81 style='height:15.75pt'>12:45-13:00</td>
  <td class=xl76 style='border-top:none'>61</td>
  <td class=xl76 style='border-top:none;border-left:none'>95</td>
  <td class=xl77 style='border-top:none;border-left:none'>Transfer, low-shot
  learning &amp; Segmentation</td>
  <td class=xl77 style='border-top:none;border-left:none'>SketchDreamer:
  Interactive Text-Augmented Creative Sketch Ideation</td>
  <td class=xl77 style='border-top:none;border-left:none'>Zhiyu Qu (University
  of Surrey)*; Tao Xiang (University of Surrey); Yi-Zhe Song (University of
  Surrey)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl75 style='height:15.75pt'>15:45-16:00</td>
  <td class=xl83 style='border-top:none'>62</td>
  <td class=xl76 style='border-top:none;border-left:none'>535</td>
  <td class=xl77 style='border-top:none;border-left:none'>Faces and gestures</td>
  <td class=xl77 style='border-top:none;border-left:none'>Security Analysis on
  Locality-Sensitive Hashing-based Biometric Template Protection Schemes</td>
  <td class=xl77 style='border-top:none;border-left:none'>Seunghun Paik
  (Hanyang University); Sunpill Kim (Hanyang University); Jae Hong Seo (Hanyang
  university)*</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>16:00-16:15</td>
  <td class=xl76 style='border-top:none;border-left:none'>63</td>
  <td class=xl76 style='border-top:none;border-left:none'>506</td>
  <td class=xl77 style='border-top:none;border-left:none'>Faces and gestures</td>
  <td class=xl77 style='border-top:none;border-left:none'>GestSync: Determining
  who is speaking without a talking head</td>
  <td class=xl77 style='border-top:none;border-left:none'>Sindhu B Hegde
  (University of Oxford)*; Andrew Zisserman (University of Oxford)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>16:15-16:30</td>
  <td class=xl76 style='border-top:none;border-left:none'>64</td>
  <td class=xl76 style='border-top:none;border-left:none'>282</td>
  <td class=xl77 style='border-top:none;border-left:none'>Faces and gestures</td>
  <td class=xl77 style='border-top:none;border-left:none'>SlackedFace: Learning
  a Slacked Margin for Low-Resolution Face Recognition</td>
  <td class=xl77 style='border-top:none;border-left:none'>Cheng Yaw Low
  (Institute for Basic Science)*; Jacky Chen Long Chai (Yonsei University);
  Jaewoo Park (Yonsei University); KYEONGJIN ANN (KAIST); Meeyoung Cha (KAIST
  &amp; IBS)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>16:30-16:45</td>
  <td class=xl76 style='border-top:none;border-left:none'>65</td>
  <td class=xl76 style='border-top:none;border-left:none'>598</td>
  <td class=xl77 style='border-top:none;border-left:none'>Faces and gestures</td>
  <td class=xl77 style='border-top:none;border-left:none'>Unsupervised Landmark
  Discovery Using Consistency-Guided Bottleneck</td>
  <td class=xl77 style='border-top:none;border-left:none'>Mamona Awan (MBZU);
  Muhammad Haris Khan (Muhammad Bin Zayed University of Artificial
  Intelligence)*; Sanoojan Baliah (Mohamed Bin Zayed University of Artificial
  Intelligence); Muhammad Ahmad Waseem (Information Technology University);
  Salman Khan (MBZUAI); Fahad Shahbaz Khan (MBZUAI); Arif Mahmood (Information
  Technology University)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl78 style='height:15.75pt'>16:45-17:00</td>
  <td class=xl76 style='border-top:none;border-left:none'>66</td>
  <td class=xl76 style='border-top:none;border-left:none'>105</td>
  <td class=xl77 style='border-top:none;border-left:none'>Faces and gestures</td>
  <td class=xl77 style='border-top:none;border-left:none'>High-Fidelity Eye
  Animatable Neural Radiance Fields for Human Face</td>
  <td class=xl77 style='border-top:none;border-left:none'>Hengfei Wang
  (University of Birmingham); Zhongqun Zhang (University of Birmingham); Yihua
  Cheng (University of Birmingham)*; Hyung Jin Chang (University of Birmingham)</td>
 </tr>
 <tr height=20 style='height:15.75pt'>
  <td height=20 class=xl79 style='height:15.75pt'>17:00-17:15</td>
  <td class=xl76 style='border-top:none;border-left:none'>67</td>
  <td class=xl76 style='border-top:none;border-left:none'>216</td>
  <td class=xl77 style='border-top:none;border-left:none'>Faces and gestures</td>
  <td class=xl77 style='border-top:none;border-left:none'>READ Avatars:
  Realistic Emotion-controllable Audio Driven Avatars</td>
  <td class=xl77 style='border-top:none;border-left:none'>Jack Saunders
  (University of Bath)*; Vinay Namboodiri (University of Bath)</td>
 </tr>
 <tr height=0 style='display:none'>
  <td width=77 style='width:58pt'></td>
  <td width=88 style='width:66pt'></td>
  <td width=61 style='width:46pt'></td>
  <td width=61 style='width:46pt'></td>
  <td width=264 style='width:198pt'></td>
  <td width=955 style='width:716pt'></td>
  <td width=2231 style='width:1673pt'></td>
 </tr>
</table>

</div>
