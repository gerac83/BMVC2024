<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>MotionMAE: Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>MotionMAE: Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">Haosen Yang (University of Surrey), Deng Huang (Meituan), Bin Wen (Beijing University of Aeronautics and Astronautics), Jiannan Wu (University of Hong Kong), Hongxun Yao (Harbin Institute of Technology), Yi Jiang (Bytedance), Xiatian Zhu (University of Surrey), Zehuan Yuan (ByteDance Inc.)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2024.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_499/paper.pdf" role="button">PDF</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_499/poster.pdf" role="button">Poster</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_499/video.mp4" role="button">Video (Right click to download)</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_499/supplementary499.zip" role="button">Supplementary</a><br></div><h2 id="abstract">Abstract</h2>Apart from learning to reconstruct individual masked patches of video frames, our model is designed to additionally predict the corresponding motion structure information over time. This motion information is available at the temporal difference of nearby frames. As a result, our model can extract effectively both static appearance and dynamic motion spontaneously, leading to superior spatiotemporal representation learning capability. Extensive experiments show that our MotionMAE outperforms significantly both supervised learning baseline and state-of-the-art MAE alternatives, under both domain-specific and domain-generic  pretraining-then-finetuning settings.
In particular, when using ViT-B as the backbone our MotionMAE surpasses the prior art model by a margin of 1.2% on Something-Something V2.  Encouragingly, it also surpasses the competing MAEs by a large margin of over 3% on the challenging video object segmentation task.<br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Yang_2024_BMVC,
author    = {Haosen Yang and Deng Huang and Bin Wen and Jiannan Wu and Hongxun Yao and Yi Jiang and Xiatian Zhu and Zehuan Yuan},
title     = {MotionMAE: Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders},
booktitle = {35th British Machine Vision Conference 2024, {BMVC} 2024, Glasgow, UK, November 25-28, 2024},
publisher = {BMVA},
year      = {2024},
url       = {https://papers.bmvc2024.org/0499.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2024 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>