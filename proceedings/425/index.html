<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>GLCM-Adapter: Global-Local Content Matching for Few-shot CLIP Adaptation</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>GLCM-Adapter: Global-Local Content Matching for Few-shot CLIP Adaptation</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">Shuo Wang (University of Science and Technology of China), Xieenlong (University of Science and Technology of China), Jinda Lu (University of Science and Technology of China), Jinghan Li (University of Science and Technology of China), Yanbin Hao (University of Science and Technology of China)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2024.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_425/paper.pdf" role="button">PDF</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_425/poster.pdf" role="button">Poster</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_425/video.mp4" role="button">Video</a><br></div><h2 id="abstract">Abstract</h2>Recent adaptations aim to boost the few-shot capability of Contrastive Vision Language Pre-training (CLIP) by transferring textual knowledge into an image recognition procedure. However, these adaptation methods are usually operated on the global view of an input image, and thus biased recognition of partial details of the image. To solve this issue, we propose a Global-Local Content Matching (GLCM) strategy, which focuses on both global and local views of the image. 
Specifically, we first extract global and local features from the input image using the CLIP visual encoder. Meanwhile, we embed the corresponding text knowledge into features by the CLIP textual encoder. Then, we construct local representation with the textual features by selectively combining discriminative local content. The local representation contains sufficient local details, and it can help the classifier to focus on the details of the image. Finally, we match the global and local content to construct a robust classifier, namely GLCM-Adapter. Our GLCM-Adapter pays attention to information from different views, and thus achieves robust recognition.
We evaluate our method on the popular few-shot classification task with 11 benchmark datasets and achieve a significant improvement over state-of-the-art methods. For example, our method achieves more than 1% average gains over the Tip-Adapter-F, and obtains more than 76.5% average accuracy for the 16-shot setting.<br><br><h2>Video</h2><center><iframe height="540" width="960" style="max-width:100%;max-height:100%;" src="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_425/video.mp4" frameborder="0" allow="encrypted-media" allowfullscreen></iframe></center><br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Wang_2024_BMVC,
author    = {Shuo Wang and Xieenlong and Jinda Lu and Jinghan Li and Yanbin Hao},
title     = {GLCM-Adapter: Global-Local Content Matching for Few-shot CLIP Adaptation},
booktitle = {35th British Machine Vision Conference 2024, {BMVC} 2024, Glasgow, UK, November 25-28, 2024},
publisher = {BMVA},
year      = {2024},
url       = {https://papers.bmvc2024.org/0425.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2024 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>