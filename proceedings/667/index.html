<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>ICAF-4: An Integrated Framework of Category-level Articulated Object Perception and Manipulation for Embodied Intelligence</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>ICAF-4: An Integrated Framework of Category-level Articulated Object Perception and Manipulation for Embodied Intelligence</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">WenBo Xu (Hefei University of Technology), Li Zhang (University of Science and Technology of China), Qiankun Li (University of Science and Technology of China), Qi Wu (Shanghai Jiaotong University), Lin Yuanbo Wu (Swansea University), Liu Liu (Hefei University of Technology)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2024.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_667/paper.pdf" role="button">PDF</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_667/poster.pdf" role="button">Poster</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_667/video.mp4" role="button">Video (Right click to download)</a><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_667/supplementary667.zip" role="button">Supplementary</a><br></div><h2 id="abstract">Abstract</h2>Articulated objects are common in humanâ€™s daily life. Current research on articulated objects often emphasizes visual understanding of articulations rather than high-level functional manipulation tasks from a single RGB-D or point cloud observation. In this paper, to study the problem of Category-level Visually Articulated object Perception task (C-VAP), we propose an Integrated Category-level visual Articulated object perception Framework, namely ICAF-4. Given the RGB and depth information as input, the ICAF-4 is capable of end-to-end processing of four mainstream tasks for articulated objects: object detection, part segmentation, pose estimation and manipulation. To support the C-VAP task, we re-annotate the rich functional grasping affordance and grasp poses by an automatic annotation generation way for two popular articulation benchmarks, ArtImage and ReArtMix, covering object-level and scene-level datasets. Accompanying the datasets, our ICAF-4 takes the part segmentation branch, pose estimation branch and manipulation prediction branch into a single forward pass. To boost the manipulation learning performance, we propose an anchor-based grasp pose estimation strategy where the "anchor" poses serve as references at multiple sizes and the grasp pose can be learned by the anchor selection and refinement process. Experiments demonstrate the superior performance of our ICAF-4 on integrating these visual tasks for articulation perception.All data and code will be made publicly available.<br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Xu_2024_BMVC,
author    = {WenBo Xu and Li Zhang and Qiankun Li and Qi Wu and Lin Yuanbo Wu and Liu Liu},
title     = {ICAF-4: An Integrated Framework of Category-level Articulated Object Perception and Manipulation for Embodied Intelligence},
booktitle = {35th British Machine Vision Conference 2024, {BMVC} 2024, Glasgow, UK, November 25-28, 2024},
publisher = {BMVA},
year      = {2024},
url       = {https://papers.bmvc2024.org/0667.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2024 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>